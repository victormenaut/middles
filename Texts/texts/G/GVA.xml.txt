

Language processing in adults
Understanding language: recognising words
6.1 The mental lexicon
In the first chapter of this book we explained that language is an arbitrary communication system.
Among other things, this means that we have to learn to recognise the sounds, spellings and meanings of individual words, and to store this information in such a way that we can call upon it when we encounter spoken or written words.
Since dictionaries also contain information about the orthography (i.e. spelling), phonology (i.e. pronunciation) and semantics (i.e. meaning) of words, terms such as‘mental dictionary’, ‘internal lexicon’ or ‘mental lexicon’have been used to refer to the internalised system of knowledge we use when we perceive or produce words.
We will use the term mental lexicon , and we will assume that there is a single mental lexicon that is used for both the perception and production of both written and spoken language.
This simplifying assumption may turn out to be incorrect — future psycholinguistic research may provide evidence that different lexicons are involved in the perception and production of language, or that different lexicons are involved in the processing of spoken and written language — but at present there are no good reasons for rejecting the simplifying assumption of a single mental lexicon.
The representation of a word in the mental lexicon is referred to as that word's lexical entry : this entry contains semantic, phonological and orthographic information about the word.
In order to understand a word, the reader or hearer must gain access to the semantic information contained in the word's lexical entry.
This process is known as lexical access .
In reading, lexical access  involves using information from a printed word to gain access to that word's entry in the mental lexicon.
In understanding spoken language, lexical access is achieved by using information from the acoustic representation of a word.
The mental lexicon is also involved in the production of written or spoken language.
In this case, speakers or writers start out with a semantic representation of what they want to say or write, and search for the corresponding phonological information (enabling one to say a word) or orthographical information (enabling one to write a word.
Although we will assume that the same mental lexicon is used whether we are reading, writing, spelling, speaking or understanding speech, we will introduce several complications.
We have seen that a mental lexicon must contain semantic, phonological and orthographic information about words.
However, we wish to reject the view that there is a single lexical entry for each word, containing all three kinds of information.
We will instead provide reasons for believing that the mental lexicon consists of separate sub-systems: one containing semantic information, another containing phonological information, and a third orthographic information.
We will also argue that it is probably necessary to make another distinction within the mental lexicon, and to think of there being separate sub-systems for input and output.
What we mean by this is that the lexical sub-system we use for recognising printed words in reading may be different from the sub-system we use for producing printed words in writing.
In other words there may be two separate orthographic sub-systems: an input lexicon (used in reading) and an output lexicon (used in writing and spelling).
Similarly, there may also be two phonological sub-systems: an input lexicon used for recognising speech and an output lexicon for producing speech.
If the mental lexicon does consist of a number of separate subsystems, then we need to explain how these sub-systems can communicate with each other, since such communication is obviously essential.
For example, if we ask you ‘What is large, grey and has a trunk?’, your ability to answer will depend on communication from the phonological input lexicon (used to understand the words we say to you) to the semantic component of the lexicon (containing information about the meaning of the words).
Having understood the question, you then have to produce  an answer.
This will involve communication from the relevant semantic entry to the phonological output lexicon, which enables you to produce the appropriate spoken form corresponding to the semantic entry, i.e. to say ‘elephant’(unless what we asked you to do was write the answer down, in which case what is needed is communication from semantics to the orthographic output lexicon).
6.2 Theories about the structure of the mental lexicon
As we will see in Chapter 9, one way in which the organisation of the mental lexicon is being investigated is by studying the kinds of language disorders which adults suffer as a result of certain kinds of brain damage.
However, although neuropsychological studies of patients are of great value, the information such studies yield must be considered in relation to studies of normal subjects.
Fortunately, various purely psychological methods have been devised for studying the organisation of the mental lexicon in normal subjects, and some of these will be discussed in this chapter.
What one hopes, of course, is to find that one comes to the same conclusions from using the neuropsychological method as from using psychological methods of investigation: and, as we will show in Chapter 9, such agreements between conclusions do actually occur.
The concepts mental lexicon ,lexical entry and lexical access provide a vocabulary which we can use when discussing how words are perceived and produced.
However, these concepts do not constitute a theory of word perception or production: they are names for structures and processes whose nature is to be explained by any theory of word perception or production.
A variety of such theories has been proposed.
Some of these theories concern a rather narrow aspect of the use of the mental lexicon; others are broad in scope, and attempt to say at least something about all the linguistic activities which depend upon using the mental lexicon.
In order to illustrate ways in which the concept of mental lexicon may be made more specific and applied to such activities as reading, spelling, speaking and understanding speech, we will discuss the most ambitious and most influential model of the mental lexicon yet developed, the logogen model, originated by Morton (1968, 1969).
6.2.1 The logogen model: first version
We write ‘first version’ because this model has recently undergone drastic revision, and a second and rather different version now exists.
We discuss this later version in the next section, but we begin with the first (and now obsolete) version of the logogen model since the evolution of the second version from the first is instructive.
The original version of the model is depicted in Figure 8.
The component labelled logogen system is a collection of logogens, one logogen for each word known to the person whose logogen system this is.
A logogen is a mechanism for collecting evidence; each logogen is specialised for collecting evidence concerning the presence of the word to which this logogen corresponds.
Logogens have thresholds: whenever the total amount of evidence exceeds this threshold level, the logogen ‘fires’.
This has two effects.
First, information about the meaning of the word in the component labelled cognitive system is accessed, since there is a pathway of communication from entries in the logogen system to semantic information in the cognitive system.
Secondly, information enabling the word to be produced (spoken or written) is transmitted to the component labelled response buffer , from which a spoken or written response can be generated.
FIGURE 8 The essential parts of the original version of the logogen model
Any logogen accepts three types of evidence:visual (contributed by the visual analysis of visual input),acoustic (contributed by acoustic analysis of acoustic input) and semantic (contributed from the cognitive system).
This latter kind of evidence is usually a consequence of the prior occurrence of a context.
For example, if one reads bread and , the logogen for butter will receive some input (collect some evidence) from the cognitive system even before the word butter is actually seen.
Although there are three sources of input to the logogen system, there is only one set of logogens, and any word has only a single logogen.
Therefore the logogen for cat is used for all the following tasks:
(a)
reading cat aloud
(b)
understanding the spoken word cat 
(c)
naming a picture of a cat, or a real cat
(d)
naming the source of a heard miaow
(e)
answering the question ‘What drinks milk and catches mice?’
Any input will contribute evidence to more than one logogen.
If the printed word cat is presented, ‘…the output from the visual analysis might include the attributes <three-letter word>, <tall letter at the end>, <initial c >, <final t >, and so on’(Morton, 1970, p. 206).
All of these pieces of evidence are relevant to cat , and so would increment the level of evidence in the logogen for cat : but they are relevant to other words too.
Detection of the attribute <three-letter word> would excite the logogens for all three-letter words, not just the logogen for cat .
The logogens for three-letter words ending with t would be even more excited.
The consequence of presenting any printed word, then, will be that a number of logogens will receive varying degrees of evidence — these logogens will be activated to different levels.
This is why thresholds are needed: when cat is presented, numerous logogens will be excited, but in every case the excitation will not be high enough for the threshold level to be reached — except for the logogen for cat , which will receive so much evidence that its threshold will be reached.
It is necessary to assume that, once a logogen has reached threshold, its activation level must decay away rapidly.
If not, the identification of subsequent words would be hindered.
For example, imagine that you have just seen cat , and the appropriate logogen has reached threshold, and activation of that logogen remains high.
Then you see a visually similar word like cot which increases the activation of cat as well as the activation of cot (because of the many features shared by these two words).
This additional activation of cat might cause the logogen for cot to be more highly activated than the correct logogen for cot :cot would be incorrectly identified as cat .
Morton (1968) therefore proposed that logogen activation ‘…decays very rapidly with time, reaching its original value in something of the order of 1 second.’
However, it is not the case that presentation of a word is assumed to leave no trace behind in the logogen system after a second or more has elapsed.
It is assumed instead that, each time a logogen reaches its threshold, the value of that threshold is lowered; and this value then slowly drifts up towards what it had been, but never quite reaches the previous level.
In the long run, the obvious consequence of this is that the more frequently a word occurs the lower the threshold of its logogen will be, and hence the less activation of the logogen needed for it to reach threshold.
Hence common words will be more rapidly identified or produced than uncommon words, and also a word which has been presented previously will be more rapidly identified than one which has not.
This is because (other things being equal) the prior presentation of that word will have slightly reduced the threshold of its logogen.
We have said that when a logogen reaches threshold ‘a response is available’; but since there are two output routes from the logogen system (to the cognitive system and to the response buffer) this term is ambiguous.
In fact Morton proposed (e.g. in Morton, 1970, p. 215) that each logogen has two thresholds, one governing communication to the cognitive system and one governing communication to the response buffer.
One reason for this proposal is that, when people read aloud, they make errors which are caused by the preceding and following semantic and syntactic context around the incorrectly read word.
This means that (during reading aloud) before a word reaches the response buffer,subsequent words must have reached the cognitive system — this being the only way that their semantic and syntactic features could come into play.
Information can then be fed back from the cognitive system to the logogen system to influence the response of this system to the word which is going to be misread.
If the thresholds from logogen system  to cognitive system were sometimes or always lower than the thresholds from logogen system to cognitive system , one could then explain subception (gaining access to the meaning of a word without being able to report the word) and also semantic errors which occur in the condition known as deep dyslexia in which single printed words are often incorrectly read as semantically related words, e.g. reading storm as thunder .
(For an account of deep dyslexia see Chapter 9.)
One important theoretical characteristic of logogens which has stimulated a great deal of experimentation is that they mediate priming effects in word recognition, i.e. the faster recognition of a word following previous recent exposure to that word.
As we mentioned earlier, once a logogen reaches threshold, its activation level does not immediately return to its normal resting level; it decays over a period of a second or so.
If the word is re-presented within that period, it will be recognised unusually rapidly, because its logogen will still be relatively strongly activated, and hence very little evidence need be collected for threshold level to be reached.
This kind of short-term priming effect (occurring over interpresentation periods of a second or so) is not the only form of priming, however.
In many of Morton's experiments, priming effects of much longer duration were observed: there can be as much as half an hour between the primer and primed, or sometimes a day or more (Scarborough, Cortese and Scarborough, 1977).
These long-term priming effects are explained, within the logogen model, by assuming that after threshold has been reached activation dies down rapidly at first over a period of a second or so, but does not quite reach the normal resting level: there follows a long period during which there is very slow decay of residual activation — a period measured in hours or even days.
On this account of long-term priming, the priming effect can be experimentally generated in a number of ways.
The detectability of a tachistoscopically presented target word (let us say the word fork ) should be improved by any of the following:
(a)
having seen the word fork previously
(b)
having heard the word fork previously
(c)
having named a picture of a fork previously
(d)
having answered the question ‘What eating utensil has prongs?’ previously.
This is because, if there is only one logogen for fork , all these different tasks will involve activation of the same logogen.
This prediction from the original version of the logogen model turns out to be wrong.
Winnick and Daniel (1970) showed that, whilst tachistoscopic recognition of a printed word was facilitated by prior reading aloud of that word, there was no facilitation by prior picture naming, or by prior production of that word in response to a definition-tasks (c) and (d) above.
This failure to find cross-modal priming was confirmed by Clarke and Morton (1983) and by Morton (1979), who failed to find facilitation of visual word identification by prior auditory presentation.
If priming effects are to be attributed to residual activation of logogens, the absence of cross-modal priming effects means that one must abandon the view that a single logogen system is used for visual and auditory word recognition, picture naming, and responding to definitions.
A revised version of the logogen model — a multicomponent version containing several different logogen systems — was therefore proposed (Morton, 1978).
6.2.2 The logogen model: second version
There are several ways in which one might explain why priming effects are not cross-modal.
For example, one might propose that the mental lexicon is divided into a semantic and a phonological system, rather than being a unified whole, and that priming occurs in only one of these systems.
On this view, however, it cannot be the phonological system in which priming occurs, because both reading aloud and answering definitions by an oral response use the phonological system (since both culminate in the subject saying a word); yet these two activities do not prime each other.
Hence, if the mental lexicon consists of a semantic system and a phonological system, priming effects must be attributed to the semantic system.
However, as you have probably realised, this will not work either.
Since reading aloud primes subsequent tachistoscopic recognition, the semantic system (if it is indeed the locus of priming effects) must be playing a part in tachistoscopic recognition; from which it follows that prior use of that system in responding to a definition should also prime tachistoscopic recognition.
Yet, as we have already seen, priming of this kind does not occur.
Therefore, one  cannot locate the priming effect in a semantic sub-system of the mental lexicon, nor in a phonological sub-system, and hence a model of the mental lexicon involving just these two sub-systems is untenable.
What other kind of lexical sub-system might be accessed when one reads a word aloud?
An obvious candidate is a visual recognition system.
Suppose then that we divide the mental lexicon into a semantic system, a phonological system and a visual recognition system (used for the visual recognition of words).
If priming effects arise because of residual logogen activation in the visual recognition system, then one can explain why priming occurs as it does.
The only way in which visual recognition of a word can be primed is by previously seeing the word.
Thus hearing a word, or producing it in response to an incomplete definition, will not prime visual word recognition.
The priming experiments can tell us something more specific about the visual recognition system used for identifying words.
First, they allow us to ask whether there is a general visual recognition system, or whether there are separate recognition systems for visually presented words and pictures.
If there is one general visual recognition system then visual word recognition should be primed by prior picture presentation; since this is not so, it can be inferred that visually presented words are recognised by a system specific to words — a visual word-recognition system .
A second question we can ask is whether the visual word-recognition system treats the uppercase, lowercase and handwritten version of a word as different (which would happen if the system operated at a relatively concrete level).
An alternative possibility is that these three versions of a word are treated as the same (which would happen if the visual word-recognition system operates at a relatively abstract level).
If the former,tree will not prime TREE; if the latter, the extent of priming will not depend on the typographical similarity of primer and target.
What is found in studies of this question is that priming is not reduced when the primer is handwritten and the target printed (Morton, 1978) or when the primer is in one case and the target is in another (Scarborough, Cortese and Scarborough, 1977).
As we will explain in the next section, the results of priming experiments like these mean that any model of the visual word-recognition system must incorporate a level of abstract letter recognition.
If long-term priming of visually presented words arises within a visual word-recognition system, not within a semantic or phonological component of the mental lexicon, then of course one needs to postulate not only a visual word-recognition system , but also an auditory word-recognition system , since hearing a word primes subsequent auditory recognition of that word.
FIGURE 9 The revised logogen model
Thus we arrive at the second version of the logogen model, as described by Morton (1978); it is depicted in Figure 9.
In this model, there are separate input logogens for recognising printed words and for recognising spoken words, a cognitive system (responsible for processing semantic information, generating context effects, and other complex linguistic manoeuvres), and an output logogen system, responsible for the production of spoken words.
The two direct connections from input logogens (visual or auditory) to output logogens are dotted and accompanied by question marks because no evidence relevant to whether or not they existed was available when this model was first described.
Although this second version of the logogen model is considerably more elaborate than the first, it is still seriously incomplete.
As Morton (1978) pointed out, a third input system (for recognising pictures) is needed: this kind of picture recognition system, which one might call the pictogen system (or, more strictly, the input pictogen system) was first discussed by Seymour (1973).
Figure 9 is also incomplete on the output side: it has a system to mediate  spoken output, but nothing to mediate written output or drawn output.
So to be complete the model should have three input systems and three output systems, plus a central (semantic/cognitive) system.
A further and very different way in which Figure 9 is incomplete is that it deals only with the processing of words and has nothing to say about the processing of non-words.
Any system which is exhaustively described by Figure 9 would be incapable of repeating nonsense words or reading them aloud or writing them to dictation.
This is because the output logogen system is solely for producing words , and the input logogen systems can only recognise words — a spoken or printed non-word would not cause an input logogen to respond, and hence non-words cannot enter the mental lexicon at all.
This may seem an unimportant omission: we are rarely called upon to deal with spoken or written non-words in the course of day-to-day linguistic activities.
For a variety of reasons, however, the failure to include mechanisms for dealing with non-words in one's model of the mental lexicon is not unimportant.
One reason has to do with learning to read.
As we discussed in Chapter 4, most 5-year-old children just beginning to learn to read already have a substantial auditory-recognition vocabulary.
It is often argued that if a child learned how to recode unfamiliar letter strings — printed words not previously encountered — into a phonological form, this would permit reading to be parasitic on an already established ability to access the semantics of a word from its phonology.
Learning how to translate unfamiliar letter strings into phonological form is just like learning to read non-words aloud, of course: and so a procedure for reading non-words aloud may be a crucial aspect of learning to read.
This procedure would play a central role in the phonological-recoding phase of learning to read, described in Chapter 4; and one of the varieties of developmental dyslexia described in Chapter 5 appears to be a consequence of difficulties in using this procedure when learning to read.
It may also be necessary to include mechanisms for the identification of non-words in order to explain skilled reading.
According to some views (e.g., Gough, 1972) skilled reading involves translating a word from its visual to its phonological representation prior to recognising it, and such translation would require the use of a mechanism which could also translate printed non-words into phonological form.
(This is because, on this kind of  view, the fact that words are familiar affects their recognition only after their phonological representation has been derived.)
Any model of skilled reading which incorporates this view can only be properly tested if the mechanism by which unfamiliar printed letter strings are translated into phonological form is described in adequate detail.
Yet another reason for the theoretical importance of considering how non-words are dealt with is that, according to some authors (e.g., Marcel, 1980; Shallice, 1981b), the visual recognition of letter strings uses the same recognition system regardless of whether the letter string is a word or non-word.
If studies of non-word reading support such a view, then any model in which the visual recognition of words is accomplished by a system which is unable to process non-words could not be correct: and, of course, the logogen model is one such model.
For reasons like these, it would seem a dangerous policy to ignore the processing of non-words entirely in one's theorising about the organisation of the mental lexicon; hence we must consider now what the mechanisms might be that people use when processing non-lexical verbal stimuli (spoken or written non-words).
The non-lexical task most intensively studied is reading nonwords aloud, and the most commonly advocated view has been that this task is accomplished by using a system of rules relating spellings to sounds.
Sometimes these are referred to as ‘letter-sound rules’, but this term cannot be a correct one, because of certain aspects of the nature of the writing system used for English.
Take the non-word CHOOPH, for example.
This contains six letters but only three individual sounds (phonemes).
Thus if the rules really were letter-sound rules, they could not be used successfully with such non-words, because there are not single letters corresponding to each of the sounds in the non-word.
Here we need the concept grapheme .
What we mean by a grapheme is the written representation of a single phoneme — so, for example, the PH in CHOOPH, is a single grapheme because it represents a single phoneme.
In fact, CHOOPH is made up of three graphemes — CH, OO, and PH.
Thus the spelling unit which maps onto the phoneme is not the letter, but the grapheme , and the term we should be using is ‘grapheme-phoneme rules’(not letter-sound rules).
Such considerations lead to the idea that the way non-words are read is by first dividing them up into their graphemes (CHOOPH-CH  + OO + PH or SLOATCH — S + L + OA + TCH) and then using a table of grapheme-phoneme correspondence rules to find the appropriate phoneme for each grapheme.
According to this theory (see e.g. Coltheart, 1978, 1984), there are just two different kinds of unit for reading aloud, the whole word (the unit used when reading aloud via the lexicon) and the grapheme (the unit used when reading aloud non-lexically).
The claim that no spelling unit smaller than the word but bigger than the grapheme is used when we read non-words aloud can be tested with non-words like gean ,geak , and gead .
In English, all words ending -ean have the ‘ee’ pronunciation for the grapheme ea.
Most words ending —eak also have this pronunciation, but not all (e.g. break ).
Most words ending —ead do not have the ‘ee’ pronunciation for ea .
These  facts will be irrelevant if no unit larger than the grapheme is used to read aloud, because a standard rule ea —‘ee’ will be applied to ea regardless of what other graphemes exist in the non-word.
If some unit larger than the grapheme is used, such as a (vowel + consonant) unit, one might expect gead to be pronounced to rhyme with bread , because the most common pronunciation for —ead is the one used for bread .
Kay and Lesser (1985) gave non-words like these to twelve skilled adult readers.
They found that the ‘ee’ pronunciation was given 96 per cent of the time with items like gean , 92 per cent of the time with items like geak , and 79 per cent of the time with items like gead .
The finding that ‘ee’ occurred significantly less often in the gead condition than in the other two conditions shows that it is not only the single grapheme that is used as a unit in reading non-words aloud.
The grapheme is the dominant unit (since, although most —ead words do not have ‘ee’ pronunciation, a large majority of the subjects gave this pronunciation) but not the only one.
Larger units play some role too.
So one cannot say that grapheme-phoneme conversion is the way non-words are read aloud.
Instead, we will use the term ‘subword-level orthographic-to-phonological conversion’ to stress the fact that the unit used is smaller than the whole word but not invariably as small as the grapheme.
Exactly the same issues arise in relation to the task of spelling non-words to dictation, and here again it appears that this not done solely by phoneme-grapheme rules, but that some role is played by units larger than the grapheme (Campbell, 1983).
Hence we refer to the process used for writing non-words to dictation as ‘subword  level phonological-to-orthographic conversion’.
Our discussion of the evolution of the logogen model has led us to the following view of the organisation of the mental lexicon.
We wish to distinguish between lexical input systems, lexical output systems, and a semantic system.
At the input-system level, we wish to distinguish between a visual word-recognition system and an auditory word-recognition system (and also a picture-recognition system, not relevant when stimuli are words).
At the output-system level, we wish to distinguish between a system for producing spoken output and a system for producing printed output (and also a system for producing drawn output, not relevant when what is to be produced is a word).
In addition, we need a procedure for subword-level orthographic-to-phonological conversion and a procedure for subword-level phonological-to-orthographic conversion.
 This entire processing apparatus is depicted in Figure 10.
This figure contains all the separate information-processing components which seem to be needed to explain how we do all the things we can do with verbal stimuli, plus arrows indicating pathways of communication between these components.
In the remaining sections of this chapter we will concentrate on two of these components: the visual word-recognition system and the auditory word-recognition system.
We have chosen these because, for the moment, we want to consider only language comprehension.
We will discuss language production in Chapter 8.
Since there has been such extensive research on both systems we have had to be selective: we have therefore confined ourselves to discussing just one line of work in connection with each.
FIGURE 10 An information processing model of language
6.3 The visual word-recognition system
The function of a visual word-recognition system is to accept, as input, information gathered from a printed or written word, and to produce, as output, an abstract identification of that word.
What we mean by abstract identification is that semantic and phonological properties of the word are not specified by the output of the visual word-recognition system.
One way of thinking about abstract identification is to think of the output as being a statement like ‘This is word number 538’.
Such a statement tells you nothing about the meaning of the word — to discover the meaning you have   to transmit code 538 to the semantic system.
Similarly, if you wanted to pronounce the word you would have to transmit the 538-code to the speech output system, so that the appropriate phonological representation can be generated.
One influential model of the visual word-recognition system has been proposed by Johnston and McClelland (1980).
The way in which printed input is translated into an abstract word-identity in their model is shown in Figure 11.
As will be seen, Johnston and McClelland propose a sequence of four processing stages: letter-position preprocessing, feature detection, abstract letter detection, and word detection.
FIGURE 11 The Johnston-McClelland model
In the first stage,letter-position preprocessing , each letter in a word is simply segregated from its background, and its ordinal position noted.
The outcome is that the word has been encoded as a sequence of unanalysed visual blobs, each labelled with its ordinal position (first, second, third, etc.) in the sequence of blobs.
In the next stage —feature detection — each blob is subjected to feature analysis.
In the example given in Figure 11, the feature analysis system detects the features curve at the top ,vertical contour , and oblique contour in the first blob, while not detecting such features as horizontal contour or curve at the bottom , since the first blob does not contain these features.
At the next stage —abstract letter detection — all twenty-six letters of the alphabet are represented by individual letter detectors.
Every feature detector in the feature-detection stage is linked to every letter detector in the letter-detector stage.
If there are, say, 20 different feature detectors, then there are 20 x 26 = 520 connections between the two levels.
Each of these connections is either excitatory (indicated by a solid line in Figure 11) or inhibitory (indicated by a broken line).
Any feature detector has excitatory links to all the letter detectors for letters having that feature, and inhibitory links to all other letter detectors.
Thus, for example, if the feature curve at the top is detected, this will excite the letter detectors for all letters with that feature (such as G, R or O), and inhibit all letter detectors for letters without that feature (such as H, E or J).
The letter detector for any letter (let us say R) is used to identify that letter regardless of its precise visual form: for example, uppercase, lowercase and handwritten forms of the letter R are all identified by the same letter detector.
This is why the letter detectors are referred to as abstract : they do not provide information about specific visual form.
It follows from this that a reader who has just identified a particular letter will not be able to tell whether it was in upper or lower case on the basis of the information from the letter detector level.
Concrete information of this sort must be supplied in some other way (e.g. by the feature detector level).
Experiments show that when words or letters are presented very briefly, subjects are in fact sometimes able to report what letter or word they have seen but not whether it was upper or lowercase (Coltheart and Freeman, 1974; McClelland, 1976; Adams, 1979; Friedman, 1980).
These experiments provide strong evidence for the existence of a system of abstract letter detectors.
However, it is difficult to see precisely how the feature detection and abstract letter detection levels could be linked together in such a way that a letter can be detected regardless of its case of presentation.
The  problem is that for some letters the majority of features which occur in upper and lowercase forms (like E and e) are different.
So if, for example, a feature like straight line at the top has excitatory connections with the letter detector for E, and curve at the top has inhibitory connections, how can the E-detector be excited by the stimulus e?
This problem has not yet been considered in relation to the Johnston-McClelland model, where it is of particular importance because in this model,inhibition is absolutely decisive .
No matter how many excitatory inputs a letter detector receives from feature detectors, if it receives even just one inhibitory input, it is switched off completely.
The letter detectors for P and R would both receive excitatory inputs from the vertical contour and curve at the side feature detectors if an R was presented.
However, the P-detector would be switched-off by the inhibitory link from the feature detector oblique contour .
It follows from this view of inhibitory input that activation at the letter detection level will be all-or-none: when a letter is presented, the detector for this letter is fully activated, and the detectors for all other letters are completely silent.
One could relax the postulate of decisiveness of inhibition, and propose instead that the activation of letter detectors was more-or-less rather than all-or-none.
This would happen if the total amount of activation in a detector were determined by the amount of excitation minus the amount of inhibition.
Thus, the stimulus R would excite the R-detector most, the P-detector and the B-detector to a considerable degree, and other letter detectors less.
This alternative has been adopted in the model of word-recognition proposed by Rumelhart and McClelland (1982) and McClelland and Rumelhart (1981).
As we will see, however, the all-or-none conception of activation leads to some very powerful predictions, including some which are counterintuitive yet supported by experimental results.
Thus at present there are good reasons to retain the Johnston and McClelland assumption that inhibition is decisive.
We now come to the final stage in the Johnston and McClelland model —abstract word detection .
All the twenty-six letters of the alphabet have links to all the words at the word detector level.
Each link is either excitatory or inhibitory, and, again inhibition is decisive: a single inhibitory input is sufficient to prevent a word  detector from being activated, no matter how many excitatory inputs it receives.
Thus, in the example shown in Figure 11, all detectors for words beginning with an R will receive excitatory inputs from the letter detector level, while detectors for all other words will be completely switched off by inhibition.
The detector for a word like REAP will also be switched off, despite its similarity to READ, because it will receive an inhibitory input from the letter detector for D-in-the-fourth-position.
Thus only one word detector — the one for READ — will be activated.
One point about this model which is unclear is this: what is it that prevents words like READY or READJUST from activating the word detector for READ?
There are various possible ways of dealing with this problem.
One way would be for the letter detector for, say, Y-in-the-fifth-position to have inhibitory links not only to the word detectors for words which have any other letter in that position, but also to those detectors for words which have no fifth letter.
This would cause READY to inhibit words with four letters or less, like READ.
However, this does not explain how READ could inhibit READY, or any other word of more than four letters beginning with the sequence READ.
This kind of inhibition could perhaps be achieved by including, amongst the letter detectors, a space detector, activated by the absence of input from the feature level.
The activation of this space detector would inhibit all detectors for all words containing more letters than the target.
This kind of end-of-word detector is discussed by Humphreys, Quinlan and Evett (1983).
The Johnston-McClelland model may seem very complicated, with its many levels, many detectors at each level, and very many interconnections between levels.
However, this model is simple enough for one to be able to make numerous predictions from it, and hence it has been easy to submit the model to experimental tests.
The model has emerged honourably from these tests; and we will now describe some of them.
In order to understand how predictions are derived from the model, one must first consider the phenomenon of backward visual masking , since the logic of the experiments depends upon using masking.
When a target word is briefly presented in a tachistoscope, and the subject's task is to report the word, the ability to do so is reduced if the word is followed by another stimulus (a backward mask ) rather than simply darkness.
There  are two different kinds of backward masking —brightness masking and pattern masking (Turvey, 1973).
In brightness masking , a homogeneous bright field is used after the target word.
The field behaves as if it had been presented simultaneously with the target display, and so reduces the contrast between target contours and their background (that is, the target contours will look grey rather than black).
If the brightness backward mask follows soon enough after the target and is bright enough, it will reduce contrast in the target display so much that the target contours will not be discriminable from their background, and so the target will not be identifiable.
Brightness masking can be thought of as operating primarily at the feature detector level in Figure 11; by reducing the discriminability of target features, it makes feature detection difficult (or  impossible , if the mask is bright enough).
In other words, the result of brightness masking is feature shortage (Johnston, 1981).
In pattern masking , the stimulus which follows the briefly presented target is not a homogeneous field, but itself contains contours, for example, a randomly arranged collection of fragments of straight lines and curves.
A pattern mask will impair the detection of the target not by causing feature shortage, but by causing time shortage .
Suppose the target is a letter.
Its features will be detected at the feature detector level, and these will cause the letter detector for the target letter to be activated.
If the subject's task is to report the letter, however, further work needs to be done: the abstract code for the letter must be generated and transmitted to the appropriate lexical output system.
Now, suppose that a pattern mask is presented during the time interval between the activation of the target's letter detector, and the transmission of the code from the letter detector level to the appropriate output system.
Since the mask is patterned, it will contain features which will be detected by feature detectors.
It is extremely likely that some of these detected features will not be present in the target, and so the detection of these non-target features in the mask will, via the inhibitory links from feature to letter level,turn off the activation in the target letter's detector .
Since the target's code has not yet been transferred to a lexical output system, there will no longer be any representation of the target anywhere.
In this way a pattern mask limits the time for which a target letter is represented in the system, and if the time limitation is severe enough to prevent the  target's code from reaching a lexical output system, then the subject will fail to report the target,even though the target was identified in the sense that its detector in the lexical input system was activated .
This account of the difference between brightness masking and pattern masking is supported by the introspections of subjects in masking experiments.
With brightness masking, subjects report that although the target appeared to be present for a reasonably long period it was too vaguely defined — its contrast was too low for it to be identifiable.
In contrast, with pattern masking, subjects report seeing a crisp, clear high-contrast target for an extremely short period of time, too short for the target to be reportable.
We have described the properties of brightness and pattern masking in order to be able to show how studies of letter and word recognition in backward masking experiments provide strong support for the model shown in Figure 11.
We will now go on to describe three of the experimental findings which provide such support.
(i) masking and the word-superiority effect
Reicher (1969) and Wheeler (1970) found that words of four letters are more accurately reported than single letters under tachistoscopic conditions, even when a forced choice technique is used to equate the chances of guessing.
This word-superiority effect is clearly important for models of word recognition: any model would have to offer an account of why, even though a word target has four times as many letters as a single letter target, the word is easier to perceive.
The effect, however, turned out to be a somewhat elusive one in subsequent research, until the conditions under which it can be obtained were elucidated by Johnston and McClelland (1973).
In their experiment, the stimulus was a brief visual presentation of a four letter word (e.g. COIN).
This was followed by two alternatives, the word itself and another differing by one letter (e.g. COIN and JOIN).
The subject's task was to decide which of these two alternatives had been presented.
In a second condition, the target was a single letter (e.g. C) and the alternatives were single letters (e.g. C and J).
The chances of guessing correctly were therefore the same in both conditions.
Johnston and McClelland used both of the types of backward mask which we have described.
With a brightness mask, no word-superiority effect was obtained: performance was equivalent for  word and single-letter targets.
With a pattern mask, however, performance was very much better when the target was a word than when it was a single letter.
The model of Figure 11 may be used to explain this pattern of results as follows.
Consider the pattern mask condition first.
When the target is a letter , it activates one unit at the letter-detector level but none at the word-detector level (all word detectors will be inhibited).
This letter detector remains active for some brief period of time, until the pattern mask is presented.
The mask activates a variety of features, at least some of which will not be features of the target letter.
Activity in the target letter's detector will therefore be inhibited (switched off).
In contrast, when the target is a word , both letter and word detectors will be activated.
The mask will then arrive and terminate activity in the letter-detector level —but it will not affect activity in the word-detector level .
This is because termination of word-detector activity can only be produced by inhibition from active letter detectors, and the mask does not activate any letter detectors: on the contrary, the mask terminates the activity of letter detectors.
Thus a pattern mask will greatly reduce the time available for a letter detector to generate and transmit a code to the lexical output systems, but will not affect the time available to a word detector.
Therefore, words will be reported better than single letters, i.e. there will be a word-superiority effect.
In the case of brightness masking, however, the situation will be completely different because, since a brightness mask has no features, it will not cause any feature-detectors to respond.
Rather, it will make both single letters and words more difficult to discriminate by its action at the feature-detector level, and so will have an equal effect on both types of stimuli.
Therefore, with a brightness mask there will be no word-superiority effect.
(ii) word masks v. feature masks
So far we have only considered pattern masks which consist of a random jumble of fragments of letters.
These are called feature masks , and, as we have seen, they will stimulate feature detectors, and hence affect letter detectors, but they will not penetrate to the word detector level.
Another kind of pattern mask is a word mask , in which the backward-masking stimulus is itself an actual word.
A mask of this kind will cause activation of detectors at the feature, letter and word levels.
It will, therefore, be capable of cutting short  the period of time available for an activated word detector to generate and transmit a code to the lexical output systems.
This means that a word mask should reduce the word-superiority effect in comparison with a feature mask; and Johnston and McClelland (1980) showed that this was the case.
You might be wondering why a word mask merely reduces rather than eliminates the word-superiority effect, since it effectively short-circuits the operation of the target word's detector.
Presumably, the reason is that even with a word mask, word targets are represented at two levels (word detector and letter detector) while letter targets are represented at only one.
Even if the time for which these representations are available is very brief (because of the word mask) two representations will still be better than one.
(iii) word masks and non-word masks
We hope that it has not escaped the reader that the predictions concerning the different effects of word and feature masks do not depend on the mask being a word ; they depend only on the mask being composed of letters .
It is the detection of letters in the mask whose occurrence is inconsistent with the current activation of the detector for the target word which inhibits the activation of the target-word detector.
The Johnston and McClelland model therefore predicts that the advantage of word targets over letter targets will be the same with a backward mask which is a word, as with as backward mask consisting of a random sequence of letters.
In other words, it will make no difference to the word-superiority effect whether the mask is a word or a non-word.
This prediction was confirmed by Johnston and McClelland (1980).
These and other findings discussed by Johnston and McClelland (1980) not only provide evidence in favour of their model of visual word-recognition, but also evidence against other types of model.
This is an essential aspect of theory testing, of course: results which are consistent with all models cannot count as evidence for any of them.
We will therefore end this discussion of the Johnston-McClelland model by briefly considering one further example of empirical evidence which supports this model, but does not support alternatives.
All the inhibitory and excitatory connections in the Johnston-McClelland model operate vertically (between one level and  another) but not horizontally (within a level).
However, some theories of visual word-recognition propose that there are horizontal excitatory links at the letter detection level, since these could potentially serve to speed the identification of words.
This is because, in English, only certain combinations of letters are allowed in the formation of words.
For example, if we saw the sequence PQZT, we would know that it could not possibly be an English word.
So, if we knew that P was the first letter of a word we would know that the second letter could only come from a small group, and that A, E, I, O, and U are the most likely candidates, H and S are less likely but possible, and F and N very unlikely, but not impossible.
(Other letters like B, C, D etc. are impossible.)
It is therefore legitimate to ask whether these intra-word letter constraints are used in visual word-recognition, or whether the identification of each letter in a word is carried out independently.
Gibson and Levin (1975) proposed that information about letter constraints reduces uncertainty and facilitates the reading of words.
However, a variety of experiments have revealed that both tachistoscopic recognition (Johnston, 1978; McClelland and Johnston, 1977; Manelis, 1974) and lexical decision time (Coltheart, Davelaar, Jonasson and Besner, 1977) are unaffected by inter-letter constraints.
There are also good reasons for rejecting the proposal of Rumelhart and McClelland (1982) and McClelland and Rumelhart (1981) that there are horizontal inhibitory connections between word detectors.
If word detectors inhibit each other, a backward mask which is a word will inhibit the word detector for a target, whilst a non-word mask will not.
In other words, superiority of word targets over letter targets should be smaller with word masks than with non-word masks if there is horizontal inhibition at the word detector level.
However, as we noted earlier, Johnston and McClelland (1980) showed that this was not so: the nature of a backward mask (word or non-word) made no difference to the size of the word-superiority effect.
Because numerous theoretical approaches to visual word-recognition postulate horizontal interconnections between detectors evidence suggesting that such interconnections do not exist serves a vital function in adjudicating between theories.
This evidence suggests that, whatever one's model of visual word-recognition is, the model should share one of the basic characteristics of the  Johnston-McClelland model, namely, that excitatory and inhibitory connections may be vertical (between-level) but not horizontal (within-level).
We have chosen to describe this model in detail because in our view it is both more complete, and better able to offer explanations of many more findings, than other existing models of visual word-recognition.
We have noted some unresolved problems for the model, however.
In addition, the model provides no account of how pronounceable non-words are read aloud, nor of how context influences word identification.
Presumably, the letter detector level is used equally when non-words or words are presented.
If so, there must be a second output route from this level to a system that converts letter information to pronunciation without going via the word level.
As for context effects, we will consider these after having discussed the auditory word-recognition system.
6.4 The auditory word-recognition system
We will again concentrate our discussion on one particular model; and we have selected the one which is probably the most fully worked-out.
This is the so called cohort model , due to Marslen-Wilson, Tyler and co-workers (see, for example, Marslen-Wilson and Welsh, 1978; Marslen-Wilson and Tyler, 1980).
This model proposes that there exists a set of auditory word detectors, closely analogous to the visual word detectors in the Johnston-McClelland model.
The way in which these detectors are used to identify spoken words, however, differs considerably from the way in which identification is achieved in the Johnston-McClelland model.
The auditory word detectors are activated by input from a spoken word, and an essential aspect of the model is that this activation begins as soon as the first sounds in the word are heard.
An alternative to this view is that identification does not begin until an entire word has been heard, so that only one word detector is activated for each word.
According to the cohort model, however, as soon as the first part of the word has been heard,all the detectors for words beginning with this initial sound are activated.
This set of words is called the word-initial cohort , since it is a set of words all having the same initial sound.
In the cohort model all words are initially potential candidates  for recognition, i.e. all word detectors are equally activated before a spoken word actually begins.
As soon as any information from a spoken stimulus reaches the word detectors, however, a process of candidate elimination begins.
For example, if the first phoneme of the word is /p/, all detectors for words not beginning with /p/ are switched off, leaving only detectors for the word-initial cohort activated.
If the next phoneme is /r/, then all detectors for words beginning with /p/ followed by some sound other than /r/ will be switched off.
At any point while the word is being spoken, the set of detectors which remain activated is known as the cohort .
As more and more of the presented word is heard, the size of this cohort will shrink, until eventually only one word detector remains activated.
This detector must, of course, be the detector corresponding to the word actually presented.
A system like this could be very efficient, since it could identify words even before they have been completely spoken.
Consider the word trespass , for example.
No English word except trespass (and its derivations and inflections such as trespasser and trespasses ) begins with the phoneme sequence tresp .
Therefore, as soon as the phoneme /p/ is heard, the cohort will have only one member left, and the word can be identified as trespass , even though its final phonemes have not yet been heard.
In fact, a system like this is optimally efficient.
Identifying the word as trespass sooner, that is before the /p/ is heard, would be inefficient because mistakes could be made (for example, choosing trespass when you have only heard tres could be wrong — the word could be trestle or tress .)
However, waiting for further phonemes after the /p/ would also be inefficient because this increases recognition time without increasing accuracy, since the additional phonemes do not provide any necessary information.
The most efficient point at which to identify a word is thus after hearing the phoneme at which the word first differs from every other word in English.
This is known as the word's recognition point ; and in the case of the word trespasses it is the phoneme /p/.
In terms of the Marslen-Wilson model, the recognition point of a word is the phoneme whose input to the word detectors shrinks the cohort down to a single word.
It is also possible to define the recognition point of a non-word: it is that phoneme at which the stimulus no longer corresponds  to any English word.
For example, the recognition point of VLEESIDENCE is very early because no English word begins VL.
In contrast, the recognition point of THOUSIDING is comparatively late — until the first I is heard the stimulus could be the word thousand .
If the subject's task is auditory lexical decision — that is, to decide whether a spoken item is a real word or a pronounceable non-word-monitoring the size of the cohort would allow this decision to be made.
Whenever the cohort reaches zero (i.e. whenever no word detectors remain activated), the decision NO can be made.
An obvious prediction, then, is that non-words with late recognition points (like THOUSIDING) will take longer to reject as words than non-words with early recognition points (like VLEESIDENCE).
Marslen-Wilson (1978) has shown this to be true.
He has also shown that, as the model predicts, the lexical decision time for non-words with early and late decision points is identical if reaction time is measured from the decision point .
One wants evidence from the processing of words , of course, if one is investigating a model of word recognition, and this is provided by the second experiment described by Marslen-Wilson (1978).
Here all stimuli were spoken words, and the subject's task was to listen to each word and press a button as quickly as possible on hearing the sound /t/ — a task known as phoneme monitoring .
The subject's reaction time was measured from the time at which the target phoneme occurred (not from the onset of the word containing the target).
Bearing this in mind, the results are perhaps surprising because they showed that when the target came early in a word, reaction times were long; but when the target came late in a word, reaction times were short.
In order to explain this Marslen-Wilson proposed that his subjects were first identifying each word they heard and then searching their phonological representations of that word for the presence of a /t/.
In other words, they were not directly listening for the sound /t/ in the speech as it came in, but rather determining that a word contains a /t/ after actually identifying the word.
Thus, the time taken to identify a /t/ will be directly related to the time taken to identify the word in which it occurs; and, as we have seen, this time depends on the word's recognition point.
When a target occurs late in a word, it is likely to occur after the recognition point.
Consequently, the word will often have been  identified before the target has even occurred, and so reaction times will be short.
In contrast, when a target occurs early in a word, it is likely to occur before the recognition point.
Consequently, such a word can be identified only after the subject has heard phonemes occurring later than the target phoneme, and so reaction times will be long.
If this is what was happening in Marslen-Wilson's experiment, the really important variable should be where the target occurred relative to the recognition point, rather than where the target occurred within the word.
Reaction times should be fast if the target phoneme occurred after the recognition point, and slow when it occurred before the recognition point.
More generally, if reaction time is plotted against the time interval between target and recognition point, the result should be a function which increases linearly.
This is the pattern which emerged from Marslen-Wilson's (1978) data.
These data are of interest not only because of the support they provide for the cohort model, but also because of a more general point they make about cognition, namely, that we must make a clear distinction between the sequence of processing stages and the accessibility of these stages for consciousness, or for the control of responses.
Analysis of the word-superiority effect in terms of the Johnston-McClelland model led to the conclusion that although word identification depends upon and occurs after letter identification, tasks requiring responses to words may  nevertheless be performed more accurately, and indeed more quickly, than tasks requiring responses to letters.
The same kind of conclusion is suggested by Marslen-Wilson's phoneme monitoring data.
A /t/ which occurs before the recognition point must be used for the purposes of recognising the spoken word: but it appears not to be used for making phoneme-monitoring decisions, since monitoring latencies are related to the word's recognition point, not to the point at which the target occurred.
The two experiments we have just described, one on auditory lexical decision and the other on phoneme monitoring, support the view of auditory word recognition as a process by which an initially large cohort of candidate word detectors shrinks rapidly in size as information from the speech signal flows in, until eventually the cohort is reduced to a single candidate.
At this moment (which occurs when a word's recognition point is reached), the word can  be identified, even if much more of the speech signal has yet to reach the word-recognition system.
6.5 Context effects on word recognition
6.5.1 Context effects on auditory word recognition
Our discussion of the cohort model is incomplete as yet, however, because we have not discussed context effects.
As a rule, people do not hear single isolated words, and in the great majority of cases any word we hear is preceded by a context which could in principle be used to assist word recognition.
For example, we noted earlier that if we hear tres …, the cohort will include trestle ,tress and trespass , and so the recognition point will not have been reached.
Suppose, however, that when we heard tres it was preceded by Forgive us our …
If the recognition of spoken words is affected by preceding context then we would expect that this highly familiar context should be sufficient to rule out trestle and tress as possible candidates, despite their consistency with the partial word tres ….
The effect of preceding context on auditory word recognition was investigated by Marslen-Wilson and Welsh (1978) using a speech-shadowing task in which the subject listened to continuous speech through headphones and repeated it aloud with as little delay as possible.
The speech input contained occasional deliberate mispronunciations — for example,tragedy mispronounced as trachedy .
Very often these mispronunciations were corrected when the subject repeated the speech, and in about 50 per cent of cases the correct pronunciation was restored with no disruption of the fluency of shadowing.
Such cases are known as fluent restorations .
If we return to our example, we can see that if a subject hears Forgive us our tres …and a decision has been made at this point that the incomplete word is trespasses , the subject can begin to say trespasses and ignore the rest of the word.
In doing this the subject will fail to detect that it was not trespasses that was presented, but the mispronunciation tresbasses .
The frequency of fluent restorations is thus an indication of whether mispronunciations occurred before or after the recognition point of a word, and thus an indication of where in a word the recognition point actually occurred.
By comparing the frequency of fluent restorations in different  conditions, it is possible to see whether preceding context affects the recognition point of a word.
If it does, fluent restorations should be more frequent when there is an appropriate context than when there is not.
Marslen-Wilson (1975) found that fluent restorations occurred far more often during the shadowing of normal prose than when the mispronounced word was semantically or syntactically incongruent with respect to the sentence containing it.
If context affects the recognition point of a word, as this result suggests, then it should be possible to show that a context which makes a word very predictable will have a greater effect on fluent restorations than a context which makes a word only fairly predictable.
Marslen-Wilson and Welsh (1978) found that fluent restorations occurred more frequently when the word was very predictable (e.g. in Still he wanted to smoke a cikarette  ) than when it was only moderately predictable (e.g., in It was his mizfortune  that they were stationary ).
Such results indicate that the recognition point of a word depends on the preceding context.
In terms of the cohort model this means that the size of a cohort at any given moment will depend not only on how much of a current word has been analysed, but also on information from the preceding context.
A very predictable word has a smaller context-produced cohort than a moderately predictable word, so less of the word itself will need to be analysed to reduce the cohort to a single candidate.
There is, thus, a higher probability that the mispronunciation will be in the unanalysed segment of the word and so will escape detection.
A similar rationale lies behind a somewhat different experimental task —word monitoring — used in the work of Marslen-Wilson and Tyler (1980).
Subjects are given a particular word — let us say the word lead — and then they listen to a pair of sentences and press a button as soon as they hear the target word.
The sentences heard were either normal prose (e.g. Some thieves stole most of the lead off the roof ) or syntactic prose (e.g. No buns puzzle some in the lead off the text ) which was syntactically correct but semantically anomalous.
In a third condition the ‘sentences’ were randomly arranged strings of words (e.g. some the no puzzle buns in lead the text off ) which were neither syntactically correct nor meaningful.
Mean reaction time for detecting the target word in normal prose was 273 milliseconds, measured from the onset of the target word.
Since, on average, the target word actually took 369  milliseconds to say on the recording subjects were listening to, this means that subjects had often pressed the button before they had heard all of the target word.
In fact, assuming that the time elapsing between a subject's decision to respond and the actual pressing of the button was about 75 milliseconds, subjects on average identified the target after having heard only the first 200 milliseconds of it.
Now, the first 200 milliseconds of a word rarely identify the word exactly, and in fact analysis of the 81 different target words used by Marslen-Wilson and Tyler showed that these first 200 milliseconds were consistent with an average of 29 different words for each target.
Put another way, this means that after only 200 milliseconds the target word cohort would have contained an average of 29 words if the word was being identified independently of any context.
However, since subjects could make accurate identifications of a word at this point, it follows that the cohort did not contain 29 items but only 1.
The only possibility is that information from the context was used to eliminate from the cohort items which were inconsistent with the preceding context.
This explanation is consistent with the finding that monitoring latencies were considerably slower in the two conditions where preceding words were of considerably less help in predicting target words.
In the syntactic prose condition the mean latency was 331 milliseconds and in the random condition the mean latency of 358 milliseconds was even longer.
(Remember that the mean latency in the normal prose condition was 273 milliseconds.)
A similar pattern emerged from a second kind of monitoring —rhyme monitoring — in which subjects were given an instruction like Press the button whenever you hear a word rhyming with ‘bread ’.
As we saw in our discussion of phoneme monitoring, phonological information (needed to decide whether a word rhymes with a target) is accessed after the word has been identified.
This means that rhyme monitoring should take longer than word monitoring because it involves an additional stage.
However, since rhyme monitoring involves word identification it will also show the same context effects as word monitoring.
The results of the two monitoring tasks in the three context conditions are shown in Figure 12.
It will be seen that there is a constant difference (about 140 milliseconds) between the word monitoring and rhyme monitoring conditions.
This suggests that subjects always identified a word first and then decided  whether or not it rhymed; and that the rhyme decision took about 140 milliseconds to make after the word had been identified.
FIGURE 12 Raw word position means for each combination of prose contexts and monitoring tasks
Further information about the operation of context effects in auditory word recognition can be obtained by examining the relationship between detection latencies and the position of the word in the context.
With normal prose there was a strong relationship — the later the target occurred in the context, the faster the response.
With syntactic prose, this relationship was present but less strongly.
In the random words condition, reaction time was unrelated to target position.
We can explain this pattern if we remember that in the normal prose condition subjects were receiving both a syntactic and semantic context.
This allowed the initial size of the cohort for target words to be reduced both on syntactic and semantic grounds.
The greater the number of preceding words making up the context, the more specific the syntactic and semantic constraints would be, and thus the greater the number of words that could be eliminated from the target word's cohort.
In the syntactic prose condition there were, of course, no semantic constraints available to reduce the size of the target word's cohort.
However, there were syntactic constraints, and it is these which are responsible for the weak link between monitoring latency and target position.
Given that cohort size is reduced both by syntactic and semantic context and by incoming sensory information, it is necessary to decide how these two influences are co-ordinated in time.
There are two possibilities: either, that contextual information is used to reduce the size of the cohort before any sensory information is received, or else contextual factors reduce the size of the cohort only after some sensory information has been received.
Marslen-Wilson and Tyler adopt the second view, proposing that: ‘the system…does not allow…contextual factors to pre-select some class of likely words even before any of the relevant sensory information has been received.’
The main reason that Marslen-Wilson and Tyler give for rejecting the idea that context can operate before any sensory information is received is one which has been used by many other theorists, including Fodor (1983).
The crux of the argument is that, since language users are essentially unpredictable, if we use contextual information very early on in the process of word identification, we will often be misled, since successive words  in an utterance are only rarely fully predictable from their prior context.
In other words, the example of Forgive us our …is not typical of the majority of speech in suggesting only one likely candidate for the identify of the next word.
In most cases, the kind of predictions about successive words which are possible from the preceding context are much less specific and would select a very large class of words, which would be of little help in discriminating between possible candidates.
On some occasions, however, contextual information might actually be unhelpful in that it might exclude the correct item.
If Forgive us our …had been followed by sins (as it is in some modern versions of the Lord's Prayer), inappropriate contextual information could slow down word identification if used too early.
What Marslen-Wilson and Tyler propose is that at the onset of a word no reduction of the cohort by context has occurred, and that initial reduction of cohort size depends solely upon sensory input, not context.
In the first stage of word recognition, cohort reduction occurs as early sensory information defines the word-initial cohort.
Only after this cohort has been defined can context come into play; words which are members of the word-initial cohort but are inconsistent with the prior context are discarded from the cohort.
At the same time, sensory information is continuing to arrive as more of the word is heard, and this is also serving to reduce the cohort.
We might ask how much of a word needs to be heard in order to define the word-initial cohort.
This is not considered in much detail by Marslen-Wilson and Tyler (1980); however, they do suggest that the acoustic-phonetic information needed to define the word-initial cohort must include the first vowel of the word.
6.5.2 Context effects on visual word recognition
Before we conclude our discussion of context effects on word recognition, we will briefly mention two experiments which show that context effects also exist in visual word recognition.
Fischler and Bloom (1979) presented subjects with a lexical decision task in which the target item was either preceded by an incomplete sentence, or by a row of Xs.
The relationship between the sentence context and the target word varied.
In one condition, the target  word was highly predictable from the context.
In a second condition it was an unlikely completion of the sentence, while remaining both syntactically and semantically consistent with the context.
In a third condition, the target word was semantically anomalous with respect to the context.
For example, with the context She cleaned the dirt from her , the targets were shoes (in the highly predictable condition),hands (in the unlikely condition) and terms (in the anomalous condition).
Results showed that the relationship between the target and context had a strong effect on performance; words which were highly predictable were responded to more quickly than words which were unlikely, while anomalous words produced the slowest response times of all.
These results could have occurred either through facilitation of responses following a predictable context, or through inhibition of anomalous responses.
Close inspection of the data showed that the context effects were due to inhibition: when preceded by an anomalous context, words took an average of 110 milliseconds longer to identify than when preceded by a row of Xs.
Furthermore, the anomalous condition produced the greatest number of incorrect lexical decisions (i.e. NO responses to real words).
Fischler and Bloom conclude from these results that in visual word recognition the effect of context is essentially inhibitory rather than facilitatory.
However, before we discuss the difference between this finding and that of Marslen-Wilson's demonstration of facilitatory effects of context in auditory word-recognition, we should note that another experiment carried out by Fischler and Bloom (1979) showed that some facilitation did occur with words which were very predictable from their context.
(These were words which could be accurately predicted by 90 per cent of subjects in a sentence completion task.)
In comparison with the inhibition effect, however, this facilitation only occurred when the subject was given a relatively long time to read the context.
With a different method of presentation, in which the subject was forced to read each word of the context very rapidly (Fischler and Bloom, 1980), only inhibition effects occurred.
If we accept the claim of Fischler and Bloom that this latter task produces effects which are more typical of those which operate in normal visual word recognition during reading, we might ask why context effects appear to operate differently in visual and auditory word recognition?
As we saw in our discussion of Marslen-Wilson and Tyler (1980), there are good reasons why word identification cannot be a largely top-down process.
Prediction from context is not normally very efficient since language is essentially unpredictable.
However, since in auditory word recognition we hear the different sounds of a word in sequence, it is possible to make use of contextual information after we have heard part of the word — in which case such information can be used effectively.
In contrast, visual word recognition does not proceed by analysing part of the stimulus.
We can have access to all of a written word at the same time, and so there is no advantage in having a visual word-recognition system which uses context to help identify words after they have been only partially processed.
However, there is an advantage in having a system which can check word identification to see if the word which we have identified is consistent with context, and it is this checking procedure which Fischler and Bloom claim is causing the inhibition effects which they have demonstrated.
In other words, in visual word recognition, preceding context is not used to predict what word is coming next, but, rather, to see whether the latest word to be identified is semantically coherent with what has gone before.
Detection of a semantic anomaly is highly informative because it indicates that an error may have been made either in understanding the context, or in perceiving the new unit.
Thus detection of a semantic anomaly forces a check to be made on the identification of a word, and this checking increases lexical decision time.
6.6 Overview
In this chapter we have discussed the recognition of spoken and printed words.
We began by considering the concept of a ‘mental lexicon’ and we described the evolution of the Logogen Model of word recognition.
We then went on to describe the Johnston-McClelland model of visual word recognition and the Cohort model of auditory word recognition.
In doing so we have argued that the processes involved in word recognition are rather different for spoken and printed words.
In particular, we have shown that the effect of context on word identification is different.
In the case of spoken words, context is used to speed up recognition by reducing  the number of possible candidates in the cohort.
Thus, in spoken word identification, context plays a part before identification has been achieved.
However, when words are identified in reading , context appears to influence word identification only after a word has been identified.
Presumably, what we might term this ‘post word identification’ effect of context is also present in the perception and comprehension of spoken language as well as written language.
However, in order to understand the many ways in which context affects language processing we need to move away from a consideration of single-word recognition to a consideration of how sentences are processed.
It is important to realise, however, that what we have discussed in this chapter is relevant to the next chapter, since if we want to know how language is understood we have to be aware of the large number of different levels of processing which are involved.
Understanding language: interpreting sentences
7.1 Introduction
In the previous chapter we discussed some of the factors which might be involved in the identification of written and spoken words.
However, as we saw in the final sections of that chapter, a consideration of single word identification leads naturally to a consideration of the larger linguistic units in which words normally occur; and hence we concluded the previous chapter with a discussion of contextual effects on visual and auditory word recognition.
As we saw, experiments by Marslen-Wilson and Tyler (1980) on auditory word recognition showed that a word can be identified more rapidly if it is predictable , that is, if there are syntactic and/or semantic constraints on the number of possible words which might occupy that particular position in a sentence.
We also saw that context can have an effect on the speed of visual word recognition, although experiments by Fischler and Bloom (1979; 1980) suggest that the effect is inhibitory rather than facilitatory, with a word taking longer to identify when preceded by a context which makes it semantically anomalous.
In this chapter we want to examine some of the factors which are involved in the stages of language understanding which occur beyond the stage of word recognition.
Traditionally, these aspects of processing have concentrated on an analysis of the way in which people understand and produce sentences .
However, there are good reasons for not limiting a discussion of language processing to sentences, since in many ways the sentence is not the most appropriate psychological unit — and, indeed, some authors have argued that it is not the most appropriate unit for linguistic analysis.
(See Taylor, 1984, for a discussion of the case against sentence-based syntax.)
Since this claim requires some justification, we will briefly discuss two of the main arguments against concentrating exclusively on sentence processing.
The first is that many important aspects of language processing occur in units that are larger than the single sentence.
Understanding what is written or spoken does not merely involve understanding the meaning of individual words plus their syntactic and semantic relationship within a sentence.
Language understanding also involves being able to relate the information in successive sentences, which is why psycholinguists have turned their attention to discourse interpretation , that is, to the comprehension of whole passages rather than single sentences.
We will describe some of the experiments on discourse interpretation in this chapter.
However, we have chosen to discuss experiments on discourse and experiments on sentence comprehension in a single chapter because we see them as being closely related.
Furthermore, there are good grounds for assuming that the distinction between sentence processing and discourse processing is by no means a clear one.
This is because while the sentence is an important unit in written language (where the boundaries of a sentence are clearly defined by the use of punctuation), it is less obviously a unit in spoken language.
This difference between spoken and written language usually becomes apparent when researchers attempt to transcribe spontaneous speech.
The single most important difference between written and spoken language is that spoken language contains many incomplete sentences, and is often simply a sequence of phrases or clauses.
As the following extract illustrates, breaking a transcription of spoken language into sentences often proves to be impossible (the + in the transcript indicates where the speaker made a short pause):
well it starts off + um + an owl + comes into his shop + and there's + just the counter + with a till and + a few + bottles an' um + pots of jam on it + an' he um + inspects the counter + an' + sort of‘tut-tut's 'cos it's + dusty + an’ goes off to get um + a small + sweeping brush an' sort of + cleans it up + an' then decides to move + um + the display over from + one side + over to the other…
(undergraduate student describing events in a puppet film)
It is perhaps ironical that the differences between spoken and written language are typically much smaller for researchers working on language than for the majority of the population.
This is because the spoken language of those who theorise about language has typically been influenced by long and constant immersion in written forms of language.
As Brown and Yule (1983, p. 14) point out,
…the speech of, say, an academic, particularly if he is saying something he has said or thought about before, may have a great deal in common with written language forms.
For the majority of the population, even of a ‘literate’ country, spoken language will have very much less in common with the written language.
It may be this unusually close relationship between spoken and written forms which has led psycholinguists to concentrate on the processing of single grammatical sentences, with the tacit assumption that the sentence is an important processing unit.
However, as transcripts of spontaneous speech like the one above illustrate, the listener who is attempting to understand spoken language is often being confronted with something other than a string of sentences.
(Such transcripts can also reveal a great deal about the processes which might be involved in language production, as we will see in Chapter 8.)
We are not suggesting, of course, that it is not legitimate to study the comprehension of sentences, or of passages which consist of discrete sentences.
Many illuminating results have been obtained from experiments which study single sentences and sententially-structured passages — indeed, the experiments which we report in this chapter have done just this.
However, we want to argue in favour of thinking in terms of something other than a sentence as being the major psychological processing unit; and, as we will see, a more appropriate processing unit is probably the clause.
We will, therefore, continue this chapter by examining some of the evidence concerning the units of language used in processing beyond the level of single word identification.
We will consider two distinct but related issues: the units involved in the perception of speech and in reading, and the units involved in the storage of written and spoken language in memory.
7.2 Units in language processing
7.2.1 The perception of spoken language
The first experimental investigations of the linguistic units involved in sentence perception were carried out using the click paradigm .
This involves presenting subjects with sentences which are accompanied by bursts of noise —‘clicks’— and asking the subjects to report the apparent location of the noise.
This may appear to be a rather strange task to use.
However, the rationale for using this paradigm as a way of investigating perceptual units was established by Ladefoged and Broadbent (1960) who showed that the position of clicks is not reported accurately, because they are heard as displaced from their original position to the boundaries of perceptual units.
Therefore, the apparent position of a click within a sentence is an indication of what the perceptual units of that sentence are.
Garrett, Bever and Fodor (1966) showed that clicks were perceptually localised at clause boundaries, and hence concluded that the clause is the main perceptual unit.
However this study, and the many others which followed it, were criticised on the grounds that subjects' reporting of the position of a click might not be a perceptual effect, but the result of memory or response bias.
A modification of the original click paradigm was therefore devised which removed the possibility of click displacement being due to these two latter influences.
This modified technique, involving the measuring of reaction time to clicks, was used by Holmes and Forster (1970) who showed that subjects were able to detect the presence of a click more rapidly when it occurred at a major constituent boundary than when it did not.
Holmes and Forster interpreted this pattern as indicating that a listener's processing load is least at clause boundaries, since reaction time should be shortest when a subject has the smallest processing load.
This interpretation has proved to be compatible with other results, including the finding that a click located near the end of a clause tends to elicit longer reaction times than a click located at the beginning of a clause (Abrams and Bever, 1969).
This suggests that processing load tends to increase near the end of a clause.
A similar conclusion emerges from a study by Caplan (1972) using another reaction time technique in which subjects had to decide whether a target word presented after a two-clause sentence  had occurred in that sentence.
Reaction time was faster when the target word occurred at the beginning of a clause, than when it was at the end of a clause.
For example, the time to decide that the target word snow had occurred was faster for sentence (a) than for sentence (b):
(a)
Because the weather is cold and damp, snow storms are expected.
(b)
Although we still have not had any snow, storms are expected.
Before we consider why processing load might be greater at the end of a clause than at the beginning, we will discuss some experiments which have been carried out on reading, from which a similar pattern has emerged.
7.2.2 Processing units in written language
The click paradigm can, of course, only be used for investigating the perception of spoken language.
However, it is possible to discover something about the relationship between clause structure and the processing of written language by using a subject-paced reading task .
In this task subjects read a passage in which only one word (Aaronson and Scarborough, 1976), or a small group of words (Mitchell and Green, 1978), are presented at any one time.
Subjects press a button as soon as they have read the currently available word or words, and this makes the next word or words available.
Since subjects are instructed to read and understand the passage, the varying times which elapse between presentation of one part of a passage and the subject's pressing of the button give an indication of the amount of time needed to comprehend that part of the passage.
In Experiment I reported by Mitchell and Green (1978), subjects read extracts from Tolstoy's War and Peace three words at a time.
All words were printed in uppercase and the end of each sentence was indicated with a full stop; but no other punctuation was used.
Results showed that the amount of time subjects spent reading each group of three words varied as a function of clause and sentence boundaries.
Subjects spent longer reading words  when they occurred at the end of a clause or a sentence.
Mitchell and Green point out that these pauses could have occurred for several different reasons.
Subjects might pause out of habit at points where it would be appropriate for them to pause when reading aloud.
Alternatively, they might pause in order to connect the material they have just read to material they had read earlier in the text.
Finally, the additional time taken to read words occurring at the end of clauses and sentences might reflect the time taken to carry out syntactic processing of that particular clause or sentence.
Mitchell and Green (1978, Experiment IV) attempted to distinguish these explanations by presenting subjects with an extract from Pirsig's novel Zen and the Art of Motorcycle Maintenance , which began and ended with a simple concrete description of the countryside.
The middle of the passage was, however, rather different since it concerned a motorcyclist's deliberations about the nature of empiricism, and it was considerably more difficult to understand.
The general pattern of pausing in this experiment proved to be very similar to that found in the earlier experiment; subjects tended to pause at clause boundaries.
However, these pauses were considerably longer for the philosophical section of the passage than for the description of the countryside.
For the latter, the additional time spent on reading words occurring at clause boundaries (in comparison with words not occurring at boundaries) was 73 milliseconds.
For the philosophical passage, the additional time was 228 milliseconds.
The finding that the length of pauses occurring in the subject-paced reading task is related to the difficulty of the material being read suggests that pause length is an indication of processing demands of some kind.
Further analysis of the relationship between pause length and syntactic complexity suggested that the additional reading time was not caused by greater syntactic complexity.
Therefore, Mitchell and Green concluded, the additional reading time must reflect the additional difficulty of relating the information currently being obtained to information presented earlier in the passage.
As we will see later in this chapter, there is evidence that relating new information to earlier information is an important aspect of language comprehension.
For the moment, however, we want to consider what might be involved in the processing of an individual  clause, and we will suggest that the additional processing load at the end of clauses arises because of several different processes, some concerned with understanding the syntax and semantics of the clause which has just been read, and others with the relating of information in this clause to earlier information in the passage.
7.2.3.
Memory for written and spoken language
In order to continue our discussion of the linguistic units involved in language comprehension, we will now review some of the experiments which have investigated the storage of spoken and written language in memory.
Like the experiments we have discussed in the two previous sections, these, too, lead to the conclusion that each sentence in discourse is processed clause by clause.
We will begin by briefly mentioning one of the first experiments to investigate memory for prose passages, as distinct from single sentences.
This was carried out by Sachs (1967) and it compared recall of sentences which had just been heard with recall of sentences which had been heard earlier in a passage.
Sachs argued that since only a limited amount of information can be stored verbatim in memory, normally only the most recently heard sentence is remembered word for word.
For earlier sentences, only meaning will be retained.
It follows from this that in a recognition test subjects will find it extremely difficult to distinguish one of the early sentences in a passage from similar sentences which mean the same.
However, if a sentence has only just been heard, subjects should be able to tell the difference between that sentence and all other sentences — even sentences which mean the same.
Sachs tested this prediction by presenting subjects with passages containing a target sentence in various positions.
As an important control, this sentence was neutral with respect to the rest of the passage, so that subjects would be unable to reconstruct it from what they remembered of other sentences in the passage.
Subjects were told that after they had heard the passage one sentence from it would be repeated, either exactly, or with some small change, and that their task was to say whether the sentence had been in the passage or not.
If we think of He sent a letter about it to Galileo the great Italian scientist as the target sentence, the kinds of changes which  Sachs introduced when the sentence was repeated were either semantic —Galileo the great Italian scientist sent him a letter about it or syntactic —A letter about it was sent to Galileo the great Italian scientist /He sent Galileo the great Italian scientist a letter about it .
Sachs compared subjects' performance with a variety of target sentence positions varying from 0 syllables delay (for a sentence which had been heard immediately before the recognition test) to 160 syllables delay (for a sentence occurring relatively early in the passage).
As expected, she found that subjects were very accurate at distinguishing a sentence they had only just heard from similar sentences with semantic or syntactic changes.
However, when subjects had heard several sentences after the target sentence, they were only able to distinction reliably between the target and a similar sentence involving a semantic change; sentences with syntactic changes were confused with the target.
Sachs therefore concluded that the syntax of sentences is held for only a short time, after which only their meaning is retained.
Sachs experiment was concerned with memory for sentences .
However, an experiment by Jarvella (1971) suggests that similar arguments can be made about memory for clauses .
Jarvella presented subjects with prose passages which were interrupted at intervals for testing of immediate recall.
The passages were constructed so that they contained a target clause which in one version of a passage formed a sentence on its own and in another version was part of a longer sentence.
For example, the third clause in the following extract (a) is part of a longer sentence:(1)The tone of the document was threatening .
(2)Having failed to disprove the charges (3)Taylor was later fired by the president .
Compare this version with the following (b) in which the same final clause forms a complete sentence:(1)The document also blamed him (2)for having failed to disprove the charges .
(3)Taylor was later fired by the president .
Jarvella was interested in comparing the accuracy of recall for clauses in these two conditions.
He found that the clause which had been heard immediately before testing was usually recalled completely accurately.
(On average recall scores were 96 per cent.)
However, recall of the preceding clause (clause 2) varied depending on whether or not that clause formed part of the same sentence as the most recent clause.
When it did not (as in version (b)), recall scores dropped to 50 per cent.
However, when the  preceding clause and the last-heard clause formed part of the same sentence (as in version (a)), recall scores for the preceding clause were 81 per cent.
Since Jarvella instructed his subjects to recall as much as they could remember word for word , and he scored for accuracy of verbatim recall, these results suggest not only that the last-heard clause has the highest verbatim recall but also that the amount of syntactic information which is being retained about a preceding clause depends on whether or not that clause forms part of a larger linguistic unit which also includes the most recent clause.
If the last two clauses heard are syntactically related to one another then sufficient syntactic information about the penultimate clause needs to be retained in order for the larger linguistic unit they form to be understood.
If we return to the example, we can see that clause (2),having failed to disprove the charges , cannot be fully understood in version (a) until clause (3) has also been heard.
However, in version (b) this same clause completes a larger unit, and so information about its syntactic structure need not be retained.
With this explanation in mind, it is interesting to look at the accuracy of recall of clause (1) in the two conditions.
We might expect that in condition B, where clauses (1) and (2) are syntactically related, verbatim recall of these clauses should be similar.
In contrast, in condition A, where clause (1) forms a complete sentence on its own, this should not be the case.
Jarvella's results show this pattern: in condition B the average recall of the first two clauses was almost the same (47 per cent for clause (1), 50 per cent for clause (2)), whereas in condition A recall of the first clause was only 29 per cent, compared with 81 per cent for the second clause.
7.2.4 The ‘clausal hypothesis'
The results of Jarvella's (1971) experiment, and those which we described in sections 7.2.1 and 7.2.2 have led several authors to propose a clausal hypothesis in order to explain sentence and discourse comprehension.
This hypothesis (originally proposed by Carroll and Bever, 1976 and Fodor, Bever and Garrett, 1974, and more recently discussed by Marslen-Wilson, Tyler and Seidenberg, 1978 and Flores d'Arcais and Schreuder, 1983) has two main  aspects.
First, it proposes that clauses are the primary units of normal speech perception in that incoming material is organised in working memory clause by clause, with the listener or reader accumulating evidence until the end of a clause.
Second, it proposes that once the end of a clause has been reached, working memory is cleared of information about the syntactic structure of the clause and the content of the clause is represented in a more abstract form.
We will consider the second claim of the clausal hypothesis first, since it relates to some of the findings which we have just been discussing.
The claim is that working memory is cleared as each clause is completed, and the results of initial processing are recoded for storage in long-term memory, with the effect that syntactic information is lost.
However, as we saw in Jarvella's (1971) experiment, this is an over-simplification.
The amount of information about the syntactic structure of a clause which is retained depends on what kind of clause we are dealing with.
For clauses which form a complete unit on their own, or which complete a larger linguistic unit, syntactic information may well be cleared from working memory once the clause has been analysed.
However, for clauses which are not ‘complete’, such purging of syntactic information is not appropriate.
Flores d'Arcais and Schreuder (1983) propose that the kind of processing carried out at the end of clauses depends on their degree of ‘completeness’.
Evidence from several studies supports this view.
For example, in a study by Flores d'Arcais (1978) subjects were asked to detect sudden changes in which ear they were receiving a spoken message.
It was found that detection latencies were generally longer when the change in ear of presentation took place during presentation of a subordinate clause than during a main clause.
However, latencies were faster for subordinate clauses when they occurred in sentence final position rather than sentence initial position.
This pattern of latencies can be explained if we assume that when a subordinate clause appears before its main clause syntactic information about the subordinate clause needs to be retained until the main clause has been heard.
This requirement increases the listener's processing load, and leads to increased response latencies on the localisation task.
However, when a subordinate clause follows a main clause this additional processing load does not occur.
Results reported by Marslen-Wilson, Tyler and Seidenberg (1978) using monitoring tasks also show that the completeness of information in a clause has an important influence on its processing.
In their experiment, subjects listened to sentences and pushed a button as soon as they heard a target word.
The target words occurred in sentences with two clauses, but the clauses varied in the extent to which the first clause could be fully interpreted without information from the second clause.
For example, in sentence (1a) the information in the first clause is relatively complete in comparison with the first clause of sentence (1b):
(1a)
Even though they are quite small cats , they need a lot of living space.
(1b)
Even though they are quite small,cats need a lot of living space.
This is because in sentence (1a) the referent for the pronoun they is provided within the first clause, whereas it does not become available until the beginning of the second clause in sentence (1b).
This means that, for both sentences, it is only the arrival of the word cats which allows the listener to complete interpretation of the first clause.
Therefore, if the completion of (what we will call for the moment) a ‘unit of information’ is important in sentence processing, rather than the completion of a clause, monitoring latencies for the word cat should not be significantly different in sentences (1a) and (1b).
However, in sentences like (2a) and (2b) where the clause boundary and the unit of information boundary do coincide, the monitoring latency for trout should be significantly different depending on whether it completes a clause (as in 2a) or begins a new clause (as in 2b):
(2a)
Although Mary very rarely cooks trout , when she does so it tastes delicious.
(2b)
Although Mary very rarely cooks,trout is one of the things she prepares well.
In order to compare processing of sentences in these two conditions, Marslen-Wilson et al.used two different monitoring tasks.
In one task —rhyme monitoring — subjects were asked to  listen out for a word which rhymed with a cue word given in advance (e.g. doubt ).
In the other task —category monitoring — subjects were asked to listen out for a word belonging to a particular semantic category (e.g. fish ).
The pattern of results obtained by Marslen-Wilson et al.was similar for both tasks.
With sentences like (2a) and (2b) in which clause boundary and information unit boundary coincided, monitoring latencies were slower when target words occurred after the clause boundary than when target words occurred before the clause boundary.
(As we explained in Chapter 6, this is because a word which is highly predictable from the syntactic and semantic context can be detected more quickly than a word which is less predictable.)
However, there were no differences in monitoring latencies for words occurring before and after the clause boundary in sentences like (1a) and (1b) where clause and information boundary do not coincide.
These findings support the view that it is not the boundary of clauses per se but the boundary of units of information which is a major determinant of the units involved in language comprehension.
In the light of experiments like the two we have just described, Flores d'Arcais and Schreuder (1983) propose a modification to the clausal hypothesis, which is that, after a clause has been presented, the continued presence of information about the syntactic structure of that clause in working memory will depend on whether such information is needed for further processing.
When syntactic information is not needed for further processing, it may be cleared from the short-term store.
As we have seen, there are good reasons for accepting this proposal.
We want to suggest, however, that clause-to-clause processing demands are not the only constraints on the storage of information about the syntactic structure of clauses and sentences.
The amount of syntactic information which is retained also depends on relations between syntactic and semantic form and pragmatic aspects of what was being communicated.
An interesting illustration of this comes from a series of experiments by Kintsch, Bates and co-workers (Kintsch and Bates, 1977; Bates, Kintsch, Fletcher and Giuliani, 1980) in which recognition memory for real-life speech was tested.
Kintsch and Bates (1977) tested students on their memory for classroom lectures several days after the lecture, when the students  were not expecting such a test.
Kintsch and Bates used a similar procedure to Sachs (1967)(see section 7.2.3), asking the students to distinguish between original sentences and paraphrases.
Surprisingly, after 48 hours, there was a significant ability to discriminate paraphrases from originals for all types of lecture material tested (topic statements, details, jokes and asides).
Even after 5 days, students could still discriminate original forms of jokes and asides.
At first sight it is difficult to reconcile this finding with those of Sachs (1967) and Garrod and Trabasso (1973) that recognition memory for the surface form of sentences disappears within 40–60 syllables of input.
However, the difference between performance can be explained if we consider the relation between surface form and pragmatic distinctions.
In normal spoken language there are often clear pragmatic constraints on the choice of particular syntactic forms.
For example, the choice of a passive rather than an active is influenced both by whether the things referred to are animate or inanimate and by what the basic theme of the speaker's current discourse currently is (Harris, 1977; 1978).
However, as Bates, Kintsch, Fletcher and Giuliani (1980) point out, the relation between syntactic form and pragmatic constraints is often ignored in the kinds of discourse which subjects are typically asked to process in psycholinguistic experiments.
Bates et al.therefore investigated recognition memory for conversation in a ‘soap opera’ and found that the surface form of an utterance was remembered if it carried some pragmatic meaning.
For example, subjects were able to remember whether a character had been referred to by name or by pronominal reference (as in,He  won't leave Rachel's room /Mac  won't leave Rachel's room ) and whether an utterance had contained a full clausal unit or an abbreviated elliptical clause (as in You mean that Jamie doesn't know that his mother is in the hospital yet ? /You mean that Jamie doesn't know that yet ?).
This suggests that the claim that all information about the surface form of an utterance is lost when it is purged from working memory needs to be modified to distinguish between surface structure which reflects underlying pragmatic distinctions, and surface structure which does not.
Surface structure which does carry pragmatic significance may well be retained.
(See Keenan, MacWhinney and Mayhew (1977) for a further discussion of this point.)
Another possible modification to the clausal hypothesis concerns the claim it makes about clause interpretation.
As Marslen-Wilson et al.(1978) have pointed out, in its strong form, the clausal hypothesis is that interpretation of a clause does not begin until the last word in the clause has been heard or read.
In its weak form, the clausal hypothesis is that some partial and preliminary hypotheses about the meaning of a clause will be developed as information about the clause is being taken in, although the definitive decision about clausal meaning could only be taken once the clause had been fully perceived.
The issue here is conceptually the same as the one which we considered in the previous chapter (section 6.4), when we discussed whether the identification of spoken words begins after only part of a word has been heard, or whether identification begins only when the whole word has been heard.
Aspects of the work of Marslen-Wilson and Tyler (1980) which we discussed in Chapter 6 (see section 6.5.1) strongly suggest that clause interpretation begins as soon as a listener hears the first part of the clause.
This is because both syntactic and semantic constraints affect the speed of word recognition even fairly early on in a clause, and, for this to happen, a listener must be building up an analysis of both aspects while proceeding through the clause.
If this were not so, contextual facilitation of auditory word recognition would occur only for words at the end of clauses.
It therefore seems sensible to reject a strong form of the clausal hypothesis as inconsistent with what we know about the way in which the syntax and semantics of clauses is analysed.
Instead, there are good grounds for believing that the listener constructs a syntactic and semantic interpretation of the input word-by-word, and that this information is used to guide the processing of subsequent words.
Marslen-Wilson, Tyler and Seidenberg (1978) take the view that this online view of language processing is not necessarily inconsistent with a weak form of the clausal hypothesis, in that once a complete information unit (which may not always coincide with clause boundaries) has been interpreted, other processing which results in the freeing of working memory then takes place.
However, as Flores d'Arcais and Schreuder (1983) point out, the relationship between these different kinds of processing remains to be investigated.
7.3 Syntactic and semantic processing
We have just shown that there is good evidence for believing that in the comprehension of spoken language syntactic and semantic analysis is continually being carried out.
So far, we have not said anything about how these two kinds of analysis might be related.
Before we do so, however, it is important to note that the issues about the point in a clause at which processing begins, and whether such processing is both syntactic and semantic, are independent.
We point this out because in order to consider the latter issue we will discuss experiments using both written and spoken presentation, even though we restricted our discussion of when processing begins to spoken language, where a relatively clear picture has emerged.
The relationship between syntactic and semantic processing has been a central concern of psycholinguistics for the last two decades.
The main question which was posed was whether semantic (and pragmatic) constraints could affect the amount of syntactic processing which was carried out in sentence comprehension.
Experiments by Slobin (1966), Johnson-Laird (1968), Herriot (1969), Greene (1970b) and Steedman and Johnson-Laird (1977) showed that the relative processing difficulty arising from complex syntactic forms was significantly reduced by semantic and pragmatic constraints.
For example, Herriot (1969) found that in passive sentences like The sister was hated by the brother it took longer to name the actor and acted-upon than in corresponding active sentences (The brother hated the sister ).
However, when there were pragmatic constraints on which of two people was likely to be the actor and which the acted-upon, there was no difference in response times to active and passive forms, and a sentence like The bather was rescued by the lifeguard was responded to just as quickly as The lifeguard rescued the bather .
These, and the results of other similar experiments, were taken as evidence that pragmatic and semantic constraints can over-ride syntactic analysis, information from which is only used if there are no other cues to interpretation.
There are two possible ways in which syntactic analysis might be over-ridden by semantic and pragmatic factors.
One way is that semantic and/or pragmatic factors might affect the syntactic analysis of a clause or sentence; that is, these different kinds of processing are interactive .
The other is that syntactic analysis  might proceed independently of semantic or pragmatic factors, but that the results of this analysis might be ignored because a plausible interpretation is derived using semantic and pragmatic information.
In this interpretation, syntactic analysis is autonomous .
We will consider evidence for these two views in the next sections.
7.3.1 ‘Autonomous’ versus ‘interactive’models of processing
The view that syntactic analysis is autonomous has been put forward most consistently by Forster and his co-workers (Forster and Ryder, 1971; Forster and Olbrei, 1974; Forster 1979).
For example, Forster and Olbrei (1974, p. 320) propose that: ‘syntactic processing is relatively independent of semantic processing, in the sense that the time required to analyze the syntactic structure of a sentence is constant, despite variations in the meaning of the sentence.’
This may seem a surprising claim in view of the earlier experiments we mentioned above (e.g. Herriot, 1969) which support an opposite conclusion; so we will consider the experimental results on which Forster and Olbrei base this claim with some care.
The subject's task in Forster and Olbrei (1974: Experiment 1) was to decide whether or not a given sequence of words formed a meaningful sentence.
Sentences and non-sentences were both seven words long.
The non-sentences consisted of a sequence of words in which the initial words formed a coherent unit (e.g. The chorus of winning loud the lions .)
This was to ensure that the sequences could not be rejected as sentences merely by reading the first words.
The sentences varied in syntactic complexity and were either plausible or implausible.
For example, the following sentences are plausible:The officials were given a warm reception /The dress that Pam wore looked ugly .
The equivalent implausible sentences — equivalent in the sense that they have a comparable syntactic structure — were The aborigines were shown a rusty invention and The aunt that Jim ate tasted foul .
The stimuli were presented visually and the subject responded by pressing a button if the sequence of words was meaningful.
Forster and Olbrei found that there was a significant correlation (.65) between the time taken to respond to a plausible sentence and its implausible equivalent, and hence concluded that the time  taken to analyse a syntactic structure was approximately constant, and independent of semantic constraints.
As the authors themselves point out, this finding does not provide definitive evidence against an interactive view of syntactic and semantic analysis.
Nevertheless, they claim that if syntactic analysis is guided by semantic cues then the relative difficulty of different syntactic forms should not be constant.
It should be pointed out, however, that the results of the Forster and Olbrei experiment do not provide evidence for a constant effect of syntactic analysis since a correlation coefficient of.65 accounts for less than 40 per cent of the variance in processing times.
Another experiment carried out by Forster and Olbrei (1974: Experiment 5) used a rapid serial visual presentation (RSVP) in which each word of a sentence is presented successively at an extremely rapid rate (16 words/second).
The subject's task was to report as many words as possible.
Forster and Olbrei presented active and passive sentences which were either logically reversible (e.g. The girl kissed the nurse ) or non-reversible (e.g. The girl kissed the photo ).
In contrast to the findings of Slobin (1966) and Herriot (1969), they found that the difference in performance with actives and passives was identical for reversible and non-reversible sentences.
In other words, differences in processing times arising from the greater syntactic complexity of the passive were not reduced by semantic constraints.
Forster and Olbrei conclude from this that syntactic analysis is autonomous, and suggest that the difference between their results and those of Slobin (1966) and Herriot (1969) could be explained by viewing the earlier results as an artefact of the tasks used.
(Slobin used a verification task in which subjects had to decide whether a sentence is a true description of a subsequently presented picture, and Herriot asked subjects to name the actor and acted-upon.)
However, it seems more justifiable to claim that the RSVP technique is not testing the normal comprehension of sentences.
Where subjects are asked to perform tasks which do involve comprehension, consistent evidence for the interaction of syntactic processing with other kinds of processing has emerged.
We have already seen that the work of Marslen-Wilson and Tyler (1980) supports an interactive view of processing in which different levels — phonological, syntactic, semantic and pragmatic — actively communicate with each other.
Experiments by Flores  d'Arcais (1978), Greene (1970) and Steedman and Johnson-Laird (1977) also support this view.
A recent study has been carried out by Warman (reported by Johnson-Laird, 1983).
In his experiments, subjects were presented with an informative sentence on a screen for 4 seconds, and then presented with a question about the content of the sentences.
Subjects had to respond ‘Yes’ or ‘No’to the question, and latencies were recorded.
Warman compared response latencies for different kinds of questions in which syntactic and semantic variables were systematically varied.
Results showed that whereas increased syntactic complexity of the question increased response latencies, the effects of such complexity could be removed by the presence of both semantic and pragmatic cues.
For example, one informative sentence used by Warman was In the park, the man saw the boy who waved at him .
Subjects took longer to answer a subsequent question like Did the boy that the man saw wave ? than one like Did the boy the man saw wave ?
This is because there are no semantic cues as to who is waving at whom, and so a syntactic analysis of the question is necessary and this is more difficult in the second version where the relative pronoun (that ) has been deleted.
However, for comparable pairs of questions in which there were semantic constraints on the identity of actor and acted-upon, complete syntactic analysis was not necessary; and here the presence or absence of a relative pronoun did not affect processing time.
Thus the time taken to respond to a question like Did the dog that the bus hit die ? did not increase when the relative pronoun was removed (Did the dog the bus hit die ?).
The results of Warman's experiments strongly suggest that in normal language comprehension syntactic processing is not autonomous, in the sense that the time taken to understand a sentence does not depend solely on the amount of syntactic analysis required.
However, there is another sense in which syntactic analysis might be independent of semantic and pragmatic analysis, and it is this which we discuss in the next section.
7.3.2 ‘Obligatory’ versus ‘optional’syntactic analysis
Although there seems to be fairly clear evidence in favour of an interactional view of sentence comprehension, there remains the question of whether syntactic analysis is optional or obligatory .
What we mean by this is whether a syntactic analysis is carried out even when it is rendered unnecessary for sentence comprehension by the presence of semantic or pragmatic factors.
This is obviously an important issue for psycholinguistic models of sentence and discourse comprehension, but so far evidence has been very difficult to obtain.
Recently, experiments have been carried out by Flores d'Arcais (1982) in which subjects' eye fixations were recorded as they read sentences containing a syntactic, a semantic, or a spelling error.
The subjects were required to understand the sentences and report any errors they noticed.
What was interesting in this study was that subjects' eye movements showed that they looked longer and more frequently at syntactic errors even when they did not actually report the errors .
This could be seen as evidence that there is a level of automatic syntactic processing which is not always used in sentence comprehension.
Flores d'Arcais and Schreuder (1983) take the view that syntactic computation seems to take place automatically, but that the extent to which the results of such computations are used in normal comprehension depends on whether or not pragmatic or semantic cues are available to aid interpretation.
In other words, syntactic analysis is always carried out but it may not be used in the  interpretation of any particular sentence.
Unfortunately, it is difficult to demonstrate that an analysis which does not have an influence on interpretation has actually been carried out.
It will require the devising of yet more ingenious techniques to provide conclusive evidence for or against the view of obligatory syntactic analysis.
7.4 Context effects in processing
We have already seen that the comprehension of individual clauses and sentences in written and spoken language can be influenced by information that is not contained within the sentence or clause itself, that is, information that is extra-sentential .
For example, in Herriot's (1969) experiment, the interpretation of a sentence was influenced by general knowledge about the likely connection between two people, such as a lifeguard and a bather.
In Chapter 3, we saw that children's comprehension of difficult sentences is  similarly influenced by their knowledge of what kinds of events are likely, and we suggested that this kind of non-linguistic knowledge plays an important role in language acquisition.
We also noted that children's interpretation of sentences can be improved by presenting a preceding context which serves to inform the child about the kind of meaning that a sentence will contain.
Dewart (1975) showed that young children's comprehension of a sentence like The duck is bitten by the monkey was facilitated by the provision of an appropriate context (poor duck ), in comparison with their level of performance when the context was inappropriate (poor monkey ).
We can therefore think of there being two somewhat different extra-sentential effects of context in language comprehension.
One effect comes from the use of general knowledge about the world — we will call this a general context effect .
The other comes from the information which has been derived from earlier parts of a discourse, and which is specific to that discourse — we will call this a specific context effect .
As we will see, the distinction between these kinds of effects is often difficult to make in practice, since processing of earlier sentences will often have involved the use of more general knowledge.
7.4.1 Specific context effects
We will begin by discussing an experiment by Dooling (1971) which looked at the effect of having a specific prior context on the speed of sentence comprehension.
Dooling argued that if an individual sentence is comprehended as part of a larger semantic unit, part of the meaning of the sentence is likely to be redundant.
He predicted that this redundancy would facilitate comprehension.
Surprisingly, however, he found that sentences took longer to understand when they were preceded by a context than when they were not preceded by a context.
Dooling argued that this was because a context can affect sentence comprehension in two different ways.
First, the presence of a context can speed up comprehension because it has prepared the listener or reader for some or all of the information contained in the sentence.
Second, a context may increase comprehension time by introducing additional processing demands, namely, the integration of the meaning of the sentence with the preceding context.
In order to distinguish these two aspects of context Dooling (1972) presented subjects with two tasks.
In one, subjects had to make a judgment about whether or not the sentence was meaningful.
In the other task, subjects had to judge whether or not a sentence followed appropriately from the preceding context.
Obviously, it is not legitimate to compare the time taken for these two different tasks, so Dooling used two different types of context for each task.
One type of context consisted of a single word and the other of a sentence.
For example, with the target sentence The baby vomited the milk one single word context was baby and one sentence context was A baby was given sour milk before her nap .
The rationale for the use of these two types of context is that they provide different amounts of prior information.
Dooling predicted that when subjects had to decide whether the target sentence was meaningful, a sentence context should make this task easier than a word context since the former provides more advance information about the likely meaning of the target sentence.
However, when subjects had to decide whether the target sentence followed appropriately from the context, a sentence context should be more difficult than a word context since it presents the subject with a more complex piece of information.
The results of the experiment supported Dooling's predictions.
In the task involving a meaningfulness judgment, performance was better with a sentence context, but when subjects had to judge whether the target sentence followed on from the context, performance was better with the word context.
In terms of natural discourse interpretation, Dooling's sentence context condition is much closer to what normally occurs than is the word context condition.
So we can assume that analysis of prior linguistic units has two effects on subsequent processing.
First, prior analysis facilitates current analysis by reducing the amount of processing needing to be done in order to determine the literal meaning of a clause or sentence.
Second, a prior context imposes an additional processing load (and hence an increase in processing time in Dooling's experiment) when a current sentence has to be integrated with information presented earlier in the discourse.
One of the main ways in which the integration of information presented in successive sentences of a discourse has been investigated is by looking at memory for discourse.
Many experiments have been concerned to show that information from successive  sentences is integrated into an organised whole; and that efficient memory for a discourse depends on such organisation being possible.
Evidence for the way in which information from successive units is integrated into an organised whole has been derived from experiments on semantic integration .
One of the best known experiments was carried out by Bransford and Franks (1971) who tested recognition memory for sets of related sentences.
One set concerned some ants going into a kitchen where there was jelly on the table.
The jelly was sweet and the ants ate it.
These ideas could either be expressed by four simple sentences (each containing one piece of information), or by a smaller number of complex sentences (each containing two, three or four pieces of information).
Subjects heard sentences from four different sets, in random order, and after a distractor task, were given a forced-choice recognition test in which they were given both sentences they had originally heard and new sentences which were constructed either by combining information from within a set, or combining information from different sets.
Bransford and Franks found that only sentences which contained one piece of information, or information from different sets, could be reliably identified as not having been heard before.
New sentences containing more than one piece of information from the same set could not reliably be distinguished from sentences which had been heard before.
Sentences containing all four pieces of information from a set were most frequently identified as having been heard before even when they had not, suggesting that the information from a related set of sentences had been integrated into one complex whole which was closest in form to a four-idea sentence.
One problem with this experiment is that it presented subjects with a very artificial task and, not surprisingly, various criticisms have been made by authors such as Hupet and Le Boudec (1977) and Schultz and Kamil (1979).
However, there are good reasons for believing that the interpretation of discourse in more natural situations does involve the integration of related pieces of information in memory.
More specifically, it has been proposed that the related ideas expressed in a discourse are organised into a coherent whole or schema — to use the concept first developed by Bartlett (1932).
The effect of schematic organisation in memory  was studied in a series of experiments, such as those carried out by Dooling and Lachman (1971), Dooling and Mullett (1973) and Bransford and Johnson (1972).
As we will see, some of these experiments used visual presentation and some used auditory presentation.
However, it is reasonable to assume that interpretation of all discourse, whether written or spoken, involves the integration of successive ideas into an organised whole.
We will begin with one of the earliest experiments, that of Dooling and Lachman (1971).
It is interesting to compare the methodology of this experiment with that of recent studies which have used more natural discourse.
Dooling and Lachman tested memory for passages which were vague and metaphorical and consequently very difficult to interpret on one reading, particularly when the overall theme of the passage was unknown.
If you have not read it before, try and understand the following passage used by Dooling and Lachman:
with hocked gems financing him/ our hero bravely defied all scornful laughter/ that tried to prevent his scheme/ your eyes deceive/ he had said/ an egg/ not a table/ correctly typifies this unexplored planet/ now three sturdy sisters sought proof/ forging along sometimes through calm vastness/ yet more often over turbulent peaks and valleys/ days became weeks/ as many doubters spread fearful rumours about the edge/ at last/ from nowhere/ welcome winged creatures appeared/ signifying momentous success
You probably found the passage difficult, and it would have been even more difficult presented one word at a time as it was in the Dooling and Lachman experiment.
However, your comprehension of the passage would doubtless have been improved if you had known that it was about ‘Christopher Columbus discovering America ’.
Half the subjects were given this information in advance but the other half were not told anything about the subject matter of the passage.
In addition, there were three conditions of presentation.
Words were either presented in the correct order (shown above), or in a completely random order.
In a third condition, words were presented in randomly ordered phrases, so that the sequence of words within a phrase (marked by /above) was maintained but the sequence of phrases within the passage was disrupted.
Recall of the passage was measured by counting the number of words correctly recalled.
(This is not, of course, a direct measure of how well the passage had been understood and, as we will see later, subsequent experiments have used more sophisticated measures of recall.)
Knowing or not knowing the title of the passage had a significant effect on recall, as did word order.
The fewest words were recalled with the random word order and the greatest number with correct word order.
However, within each word order condition subjects recalled more words from the passage when they had been informed of its title.
This difference cannot be attributed to more enlightened guessing when subjects knew the title because recall of words closely related to the title and of words not closely related to the title was equally improved.
Dooling and Lachman argue that the better performance of the subjects who were told the title of the passage reflects their greater ability to organise the words in the passage in memory.
Their results also show that this effect is independent of syntactic constraints, since there was no interaction between word order and knowledge of the title.
In a later experiment, Dooling and Mullett (1973) attempted to determine the locus of the thematic title effect.
They argued that the higher recall scores in the earlier experiment could have occurred because of more efficient storage of the passage at input, or because of better reconstruction of the material at recall.
They, therefore, introduced a third condition in which subjects were given the title of the passage after having read it.
In this new condition subjects had the benefit of knowing the thematic title at recall but not at the stage of encoding.
The results showed that knowing about the theme of the passage only at recall did not improve performance.
There was no significant difference between the number of words recalled in this condition and the number recalled by subjects who were not given any information about the theme of the passage.
As in the previous experiment, significantly more words were recalled when the title was presented before the passage.
A similar finding emerged from an experiment by Bransford and Johnson (1972).
This differed from the two experiments we have just described in several ways.
Subjects listened to the passage, which was read with normal intonation in order to provide information about sentence and clause boundaries.
(In the Dooling experiments no punctuation was supplied so this information was  not available.)
The passage itself was less metaphorical and the contextual information which Bransford and Johnson supplied was a picture rather than a title.
In spite of these differences the passage was still difficult to understand without knowing in advance what it was about, as you can judge from the following extract:
If the balloons popped the sound would not be able to carry since everything would be too far away from the correct floor.
A closed window would also prevent the sound from carrying since most buildings tend to be well insulated.
Since the whole operation depends on a steady flow of electricity a break in the middle of the wire would also cause problems.
Of course the fellow could shout, but the human voice is not loud enough to carry that far.
Subjects were given varying amounts of contextual information.
Some received none.
Others were presented with a picture depicting the strange juxtaposition of events in the passage before hearing the passage, while others were presented with the picture after hearing the passage.
Another group received partial information about the events described in the passage.
The complete context picture depicted a young man standing in the street serenading his girlfriend who is looking out of the window at the top of a tall block of flats.
In order to make his song heard the young man is singing into a microphone wired to a loudspeaker.
This is supported in the air, level with the girl, by several balloons.
The partial context picture showed the young man, the girl balloons, microphone, loudspeaker and the block of flats but not their unique relation in the story.
Subjects were asked to rate the passage for comprehensibility and to recall as much of it as they could.
Recall was scored in terms of the number of ideas expressed in the passage which a subject was able to remember.
(This is obviously a better way of measuring recall than merely counting the number of words remembered, although it does have the disadvantage of being a less objective method since the actual wording used by a subject may be very different from that used in the passage.)
Both comprehensibility ratings and recall scores were highest when the subject had seen the complete context picture before listening to the passage.
Provision of the context picture after presentation of the passage, or provision of a partial context, had little effect on either ratings of comprehensibility or recall.
Essentially similar results were obtained from other experiments reported in the same paper using different passages and verbal contexts.
These results suggest that knowledge of the overall theme of a passage is useful only if it is available at the time the passage is read or heard.
However, Thorndyke (1977) has argued that the experiments by Bransford and Johnson (1972) and by Dooling (Dooling and Lachman, 1971; Dooling and Mullet, 1973) do not reflect the processes normally involved in understanding and remembering discourse.
His objection to these studies was that the passages these investigators used made very little sense unless subjects did have some prior knowledge of their content.
Of course, the passages had been constructed with precisely this intention.
However, in real life such referentially obscure and ambiguous material is rare and Thorndyke therefore investigated memory for passages which were relatively coherent even when their precise theme was not specified in advance.
Thorndyke's approach to the study of discourse interpretation was rather different from that adopted in earlier studies.
It has its roots in the study of the grammar of stories, or story grammar , as this approach is often known.
There is not space here to discuss the story grammar approach in detail, and readers who are interested in this topic are advised to read the recent reviews of this and alternative approaches contained in Brown and Yule (1983: Chapter 3) and Johnson-Laird (1983: Chapter 14).
For present purposes it is sufficient to note that the story grammar approach is an attempt to characterise the structure of stories by a set of rules.
Some of these rules are concerned with the different components which go to make up a traditional story — the kind of story which is orally handed down from generation to generation.
Thorndyke argued that such stories are easy to remember because they have a particular structure.
Two aspects of this structure are relevant to the present discussion.
First traditional stories have a high degree of internal organisation in which the various sub-sections of the story plot are presented in a logical order.
Second, information about the overall theme of the story is given near the beginning.
This information about the theme is very similar to the thematic titles used by Dooling.
Thorndyke argued that both the internal organisation of the story and knowledge of the theme are of assistance in remembering the information contained in the story.
This is because both assist a reader or listener to relate individual events and pieces of information into an organised framework.
For example, if we consider one of the stories which Thorndyke (1977) used we can see that it has a high degree of internal organisation.
Furthermore, the theme of the story is made clear early on:
(1) There once was an old farmer (2) who owned a very stubborn donkey.
(3) One evening the farmer was trying to put his donkey into its shed.
(4) First the farmer pulled the donkey (5) but the donkey wouldn't move.
(6) Then the farmer pushed the donkey (7) but still the donkey wouldn't move.
(8) Finally the farmer asked his dog to bark loudly at the donkey (10) and thereby frighten him into the shed.
(11) But the dog refused.
(12) So then the farmer asked his cat (13) to scratch the dog (14) so the dog would bark loudly (15) and thereby frighten the donkey into the shed…
Thorndyke presented subjects with four versions of this passage.
In the story condition the passage was presented in the original version shown above.
The narrative after-theme condition was identical except that sentence (3) which explains the theme of the passage was given at the end.
In the narrative no-theme condition sentence (3) was removed altogether.
In a fourth condition subjects were given a description in which not only was sentence (3) removed but also the internal structure of the story was disrupted by altering the temporal sequencing of events and removing the causal connection between them.
Thorndyke predicted that this last condition would be the most difficult to understand and to remember and that the story condition would be the easiest.
The two narrative conditions were expected to be of intermediate difficulty with the after-theme condition being easier than the no-theme.
The passages were presented either visually, one line at a time, or auditorially.
In the latter case the passage was read at slightly slower than normal reading rate with care being taken to ensure that inflections or intonation did not carry across sentence boundaries.
At the end of presentation subjects rated the passage  for comprehensibility and then wrote down as much of it as they could remember, as close to verbatim as possible.
In spite of this instruction Thorndyke actually followed the procedure of Bransford and Johnson (1972) and scored the amount of information recalled irrespective of whether the exact wording was remembered.
In order to do this Thorndyke split the two passages he used into propositions .
In order to give you some idea of the boundaries of the propositions these are numbered in the extract given above.
As you would expect, the results of Thorndyke's experiment showed that both comprehensibility and recall were highest in the story condition and lowest in the description condition with the two narrative conditions producing intermediate scores.
For our present discussion, the most interesting finding concerns the difference between these two narrative conditions: both comprehensibility and recall were better in the after-theme condition than in the no-theme condition, although the recall difference did not reach significance.
This finding conflicts with those of Bransford and Johnson (1972) and Dooling and Mullett (1973) who found that providing information about the theme or context of a passage after it had been presented did not improve performance, and so concluded that such information only affected the organisation carried out when the passage was memorised.
However, as has already been pointed out, Thorndyke's study differed from the two earlier ones in that he used passages which maintained clear temporal sequencing, and clear clausal connections in both narrative conditions.
Therefore, it was possible for subjects to carry out a certain amount of integration of information even when they did not know the theme of the passage.
This degree of organisation was sufficient to allow them to remember a large part of the information in the passage so that when the theme was finally presented the remembered information could be re-structured.
In contrast, in both of the earlier studies structural integration was virtually impossible without knowing the overall theme, so that most of the information contained in the passage was lost by the time the important contextual/thematic information was supplied.
Thus, providing such information after presentation of the passage was of little assistance.
An interesting insight into the kind of restructuring which had occurred in memory when subjects discovered what the theme of  the passage was in Thorndyke's experiment comes from recall in the narrative after-theme condition.
75 per cent of subjects in this condition included a statement about the overall theme of the passage at the beginning of their recall protocols even though this information had actually been presented at the end of the passage and subjects were instructed to recall the passage verbatim.
7.4.2 General context effects
One aspect of the integration of information presented in discourse which Thorndyke's experiment ignores is that the listener or reader may also call on more general knowledge in order to integrate information into an organised whole.
This more general knowledge concerns both what the speaker or listener may know about the discourse topic, and what he/she knows about conventions which are normally followed in speaking and writing.
The ability to draw on such knowledge is an essential aspect of discourse interpretation and it is often employed by subjects in the kind of memory experiments which we have just described.
We noted that in Thorndyke's (1977) experiment the difference between recall scores in the two narrative conditions did not reach significance.
One reason for this is probably that subjects who were not told the overall theme of the passage until the end were able to draw on their knowledge of similar stories which they had heard before.
When we have attempted to replicate Thorndyke's finding with undergraduate students we have invariably found that subjects are very good at guessing the overall theme even when they are not explicitly told what it is.
The relation of information in a particular discourse to information which the subject already possesses about the discourse topic has also been investigated in memory studies.
We will just mention one study, that of Pompi and Lachman (1967).
This showed that after reading a prose passage with a clear and colourful theme, subjects tended to make false positive recognition errors which were elaborations of the theme of the passage.
For example, after reading a passage about a battle during a war, subjects falsely recognised words such as rifle and colonel as having occurred in the passage.
(A similar finding emerged in the Dooling and Lachman (1971) experiment discussed earlier where more  words closely related to the theme of the passage were incorrectly recalled when subjects were told the title of the passage before being presented with it.)
The process of schematic integration which occurs in discourse interpretation thus involves not only the relation of ideas expressed within the discourse, but also the relation of specific information contained in the discourse to the pre-existing knowledge of the discourse topic which the listener or reader has.
Another way in which pre-existing knowledge is important in discourse interpretation is in the drawing of inferences and we will end this chapter with a brief discussion of some of the research on this very complex topic.
7.5 Drawing inferences
The fact that the interpretation of a sentence often requires a listener or speaker to go beyond literal meaning was first highlighted in psycholinguistics by Johnston, Bransford and Solomon (1973).
Their argument was that when sentences are interpreted people draw inferences from the explicit information.
The drawing of inferences is an essential part of discourse interpretation because very often essential information is implied rather than being made explicit.
For example, suppose that you are presented with the following passage, used by Johnston et al.:
The river was narrow.
A beaver hit the log that a turtle was sitting on and the log flipped over from the shock.
The turtle was very surprised by the event.
When you read this you may have made the inference that the turtle was knocked into the water when the beaver hit the log.
Johnston et al.argued that such inferences are stored in memory along with the information explicitly contained in the passage, with the result that when subjects are given a recognition test they will falsely identify the inference as having occurred in the passage.
This was what happened.
The sentence A beaver hit the log and knocked the turtle into the water was falsely identified as a sentence from the passage.
In a control condition with a passage which was identical except that the turtle was ‘sitting beside ’ the log (where the inference  that the turtle fell into the water would have been inappropriate) there was no such false recognition.
The inferences involved in the Johnson et al.experiment are what we might think of as optional inferences.
Thorndyke (1976) has argued that sometimes inferences are necessary in order to link successive sentences.
We will call inferences of this type bridging inferences in order to distinguish them from all the possible inferences which could be drawn from a particular sentence.
Thorndyke attempted to compare memory for optional and bridging inferences.
He devised passages containing critical sentences which could give rise to several possible inferences.
Consider a sentence like The school teacher swung her hand at little Mary who was misbehaving .
Possible inferences might be that the teacher was angry, that she actually hit Mary, that she hurt her, or that she swung her hand and missed.
Thorndyke inserted another statement further on in the passage which was either neutral with respect to possible inferences or which supported particular inferences.
For example, a neutral statement following on from the sentence about Mary and her teacher was Mary heard some birds singing outside the classroom window .
This does not require any bridging inferences to link it to the earlier sentence.
However, complete interpretation of the statement Mary noticed that her lip was bleeding requires the bridging inference that the teacher actually struck and hurt Mary.
The potential inference that Mary was not actually hit would be disconfirmed by the same statement.
Other possible inferences, such as that the teacher was angry, would remain appropriate but would not be essential for interpretation.
Thorndyke argued that bridging inferences should be stored as an integral part of a subject's memory of a passage.
Potential inferences which were inconsistent with subsequent information should be rejected and therefore not remembered.
Potential inferences which were neither confirmed nor disconfirmed by subsequent information might or might not be remembered.
He tested this prediction by presenting subjects with a recognition test consisting of statements from the passages together with the three types of inference.
He found that in the neutral condition, where no particular inference was confirmed or disconfirmed by subsequent information, all potential inferences were falsely identified as having occurred in a passage about 25 per cent of the time.
In the other condition, where later information required the drawing of  bridging inferences, such inferences were falsely recognised 58 per cent of the time.
Potential inferences which were disconfirmed by a later statement were invariably rejected, and inferences which were neither confirmed nor disconfirmed were falsely identified as often as they had been in the neutral condition.
One question which arises from the conclusion that bridging inferences are falsely identified as having been present in a passage concerns when such inferences are drawn.
One difficulty with testing recognition memory is that inferences might be made only at the time of the recognition test.
For this reason, more on-line measures of processing have also been used to study inferences since this allows some measure to be taken of when inferences are made.
Haviland and Clark (1974) investigated the processing demands involved in drawing inferences by measuring reaction time in a comprehension task.
They compared performance in two conditions.
In one — the direct antecedent condition — a target sentence had a direct relation to a context sentence presented immediately before it.
In the other indirect antecedent condition the information in the target sentence could only be related to the context sentence by the making of a bridging inference.
For example, the target sentence The beer was warm was preceded by the context We got some beer out of the trunk in the direct antecedent condition and by the context We checked the picnic supplies in the indirect antecedent condition.
In the latter case it is necessary to make the bridging inference that the picnic supplies included beer in order to relate the context and target sentences.
Haviland and Clark found that the time taken to understand the target sentence was significantly greater in the indirect antecedent condition (1016 milliseconds) than in the direct antecedent condition (835 milliseconds).
However this difference might not have reflected the additional time taken to draw an inference in the indirect antecedent condition, but rather that there was repetition of a noun in the direct antecedent condition but not in the indirect antecedent condition.
Haviland and Clark (1974) therefore devised pairs of context and target sentences in which the amount of repetition was the same in the direct and indirect antecedent conditions.
This was achieved by repeating one noun in the indirect antecedent condition as in the following:Ed wanted an alligator for his birthday .
The alligator was his favourite present .
Linking  these sentences requires the inference that Ed received an alligator for his birthday.
Haviland and Clark found that the reaction time difference between the two conditions remained and therefore concluded that the additional time taken to comprehend the target sentence in the indirect antecedent condition arose because of the need to draw a bridging inference.
7.6 Overview
If you have read chapters 6 and 7, it cannot have escaped your notice that whereas research into the recognition and comprehension of single words has centred around detailed theories which attempt to provide a comprehensive description of all aspects of single word processing, research into the comprehension of sentences and discourse is much more fragmented.
The main reason why there is currently no overall theory of language comprehension is that the relatively simple models which have been empirically tested have proved to be inadequate.
An example of this kind of theoretical inadequacy is well illustrated by the research into the clausal hypothesis of language processing which we discussed in this chapter.
Early work on this topic supported a fairly strong version of the clausal hypothesis, according to which discourse is processed at the single-word level until a clause boundary is reached, at which point processing across the entire clause is instituted.
Once an integrated representation of the clause as a unit is created, the specific words making up the clause can be discarded from memory to make way for the words of the next clause.
As we saw in this chapter, more recently it has been found that this account of processing cannot be correct for at least three reasons:(a) there is evidence that syntactic and semantic processing is not delayed until the end of the clause;(b) there is evidence that information about the specific wording is retained after the end of a clause if that clause contains nonspecific words which subsequent clauses will disambiguate;(c) specific wording will also be retained if it has pragmatic significance.
Another way in which initially simple models of language comprehension have had to be modified is illustrated by another topic we discussed in this chapter, namely, the relation between  syntactic and semantic processing.
We argued in this chapter for an interactive model of these two kinds of processing rather than for a model in which syntactic and semantic analyses are carried out independently.
We made a somewhat similar point in Chapter 6 when we showed that the identification of single words in both spoken and written language is influenced by contextual factors.
This suggests that it is not only at the levels of syntactic and semantic analysis that language processing is interactive.
Phonological analysis also interacts with syntactic and semantic analysis.
There is also evidence in favour of the view that what we might think of as pragmatic factors influence language comprehension.
Here, we mean by ‘pragmatic’ those aspects of language processing which stem from general knowledge about the world and about the way in which people normally convey information in language.
In this chapter, we illustrated such pragmatic influences on processing by discussing context effects.
As we saw, discourse processing is influenced by context in at least two ways.
There are effects of specific context (for example, the piece of discourse currently being processed has to be fitted into and has its processing guided by the preceding and already-processed discourse); and there are effects of general context (for example , the listener/reader can draw upon his background knowledge of what the world is like to guide his discourse interpretation).
We concluded this chapter by discussing another aspect of language comprehension which forms a major part of pragmatic processing, namely the way in which understanding of what is spoken or written often involves going beyond an interpretation of what is explicitly stated.
We distinguished between optional inferences (not required for understanding discourse but naturally suggested once a discourse interpretation has been decided upon) and bridging inferences (necessary for establishing the links between apparently unrelated elements in the discourse).
What emerges from our discussion of these different aspects of sentence and discourse comprehension is that only a very complex model will suffice to explain the inter-relation of all the different factors which appear to form a part of language comprehension.
This is a theme which will re-emerge when we consider acquired disorders of language in Chapter 9.
However, we now wish to complicate the picture of language processing even further by considering what is known about the way in which speakers produce language.
Producing language
8.1 Introduction
So far in our discussion of adult language processing we have been concerned mainly with language perception and comprehension.
This is not because language production is an unimportant aspect of language processing, but simply because almost all the research into normal adult language use has been on comprehension rather than production.
As we saw in Part II, this has not been the case with child language, where both comprehension and production have been extensively studied, with comparisons being made between the child's spontaneous use and understanding of particular linguistic forms.
As we will see in Chapter 9, language production has also been a major area of study for researchers concerned with acquired language disorders in adults.
Indeed, it is possible to argue that in both the fields of child language and acquired disorders the earliest studies to be carried out were almost exclusively concerned with production.
Why, then, have the majority of studies of normal adult language processing been concerned with language comprehension?
One answer is that studies of comprehension are generally very much easier to carry out in a controlled way than studies of spontaneous production.
When subjects are required to press a button as soon as they are able to understand a sentence, or when subjects are asked to recall a passage, it is clear what constitutes a ‘correct’ response.
It is therefore possible to take completely objective measures of performance such as number of errors, reaction time, or number of correctly recalled propositions.
In studies of spontaneous speech, it is not possible to predict precisely  what a subject may say.
Therefore, the traditional methodology of cognitive psychology, which has typically been developed in order to investigate performance in tasks where correct and incorrect performance can be clearly distinguished, is often inappropriate for studying language production.
However, as we will see in this chapter, it is possible to carry out systematic studies of production; and the results of such studies have an essential contribution to make to our understanding of language processing.
The production of single words is easily enough described in the context of the information-processing model developed in Chapter 6.
All that is required is retrieval of an item from an output lexicon (phonological output lexicon for spoken production, or orthographic output lexicon for written production).
The next step up in complexity is the production of sentences.
One of the people who has been developing an information-processing model intended to describe how production of spoken sentences is achieved is Garrett (1975, 1980, 1982).
We will present an outline of the ideas behind this model.
8.2 Garrett's model of sentence production
This model is set out in schematic form in Figure 13.
We will discuss how the model works by considering how one might go about producing a sentence to describe a particular event, the event taking place in Figure 14.
If you wanted to tell someone what was going on here, there are many different sentences that you might use.
‘A boy is hitting a girl with a flower’ or ‘A girl is being hit with a flower by a boy’or ‘There's a girl, and a boy is hitting her with a flower’are all possible examples, and there are many others.
So it is obviously quite implausible to propose that sentence production involves selecting , from a set of pre-existing sentences, the one you want to produce, because there are far too many different possible thoughts one might want to convey, and far too many different ways of conveying each thought.
Sentence production must be a process of constructing , rather than selecting , sentences.
FIGURE 13 Garrett's model of sentence production
FIGURE 14 Picture of a boy hitting a girl with a flower
Any theory of sentence production, then, will be a theory which describes what the constructive processes are which, when applied to some thought which a speaker wishes to convey to someone else, result in the formulation and production of an appropriate sentence.
Garrett's model offers one view as to what these constructive processes might be as far as the production of spoken sentences is concerned.
According to his model, sentence production consists of transforming an initial message-level representation (roughly speaking, a non-linguistic representation of the basic idea the speaker wishes to convey) into a final articulatory representation (a pattern of movements of the speech musculature occurring as the constructed sentence is actually spoken aloud).
Intervening between these two levels of representation are three others, so the whole process of sentence construction involves a sequence of five different levels of representation.
The model is meant to describe what operations the speaker performs upon each level of representation to generate the next level.
We will discuss each of the five levels in turn, and describe the operations performed upon them.
The first level in Garrett's model is the message-level representation .
As we have said, this is a non-linguistic conceptual representation of what the speaker wants to convey in a sentence — the idea, in our example, that there was a boy, a girl and a flower, that the boy hit the girl, and that he used a flower to do it.
In order to create the next level of representation (the  functional level, described below) two distinct procedures are needed:
(a)
Lexical  retrieval  : the lexicon is searched, and retrieved from it are abstract codes representing the content words (nouns, verbs, adjectives, some adverbs) that the message level representation specifies — in our example, codes representing the words boy ,girl ,flower , and hit .
(b)
Creation of a functional predicate/argument structure .
In a sentence like ‘The boy hit the girl with a flower’, one can think of the verb as a predicate which has a number of arguments — that is, one could represent the sentence as HIT (BOY, GIRL, FLOWER).
Different verbs will require different kinds of predicate/argument structure.
HIT always needs at least three arguments (the hitter, the person hit, and the thing used to hit — we might call these the AGENT, the ACTED-UPON and the INSTRUMENT).
Verbs like SLEEP have only one argument (the AGENT).
So whatever verb is chosen will determine what functional predicate/argument structure is created — for HIT, it will be HIT (AGENT, GOAL , INSTRUMENT) because part of the lexical specification for the verb HIT is that it must have three arguments and they must be AGENT, ACTED-UPON, INSTRUMENT.
We should emphasise at this point that Garrett is much less specific than we are being in describing the model.
For example, he does not commit himself, as we have, to a particular account of what the predicate/argument structure is like.
Here and elsewhere we have been more committed than he in the interests of clarity of exposition.
Lexical retrieval yields three noun codes.
Creation of the predicate/argument structure yields a structure with three arguments.
When each noun code is matched up with its appropriate argument (e.g. when it is established that the boy is the agent, the girl the acted-upon, and the flower the instrument), we have reached the second level of representation of the sentence —functional-level representation .
The functional-level representation is linguistic, unlike the previous level.
A sentence's functional-level representation specifies in linguistic terms what is being done, by whom, to whom, and with what, if all of this information is contained in the message-level representation.
But specific details of sentences are not  represented.
No distinction is made, for example, between active versus passive sentences.
Whether you are going to say ‘The boy hit the girl with a flower’ or ‘The girl was hit by the boy with a flower’makes no difference at this level, because these two sentences have exactly the same functional-level representation.
If one represents the sentence as, roughly speaking, HIT [boy = agent, girl = acted-upon, flower = instrument]then clearly this representation applies just as well to the active and to the passive sentences we have just given (and to many other sentences too, such as‘The boy hit the girl, and he used a flower to do it’ or ‘There was boy, and he had a flower, and he hit the girl with it’).
Since eventually the speaker must produce one or other of these approximately interchangeable sentences, the next procedure must be to operate upon the functional-level representation in such a way as to decide upon the details of the specific sentence that is to be produced — to commit oneself, for example, to a passive or an active sentence.
We discuss later in the chapter (see section 8.4) the kinds of factors that determine whether a speaker decides to use an active or a passive sentence and factors that determine other such decisions about sentence structure.
In Garrett's model, the specific form of a sentence is represented at positional-level , which is generated from functional-level representation.
To produce positional-level representation from functional-level representation of a sentence, once again two distinct procedures are needed:
(a)
Lexical retrieval .
This is the ‘second pass’ through the lexicon.
Its function is to retrieve the phonological forms of the content words (nouns, verbs, adjectives or adverbs) whose abstract codes were selected from the lexicon earlier on.
(b)
Creation of a planning frame for the sentence .
If you decide to use an active sentence to describe Figure 14, you might create a frame such as‘The N V-ed the N with a N’: this is the right kind of frame for an active sentence with three nouns and a verb.
The selection of a particular planning frame commits the speaker to using certain function words and affixes and not others.
For example, if Figure 14 is to be described by an active sentence in the present tense (e.g. ‘The boy is hitting the girl with a flower’) the main verb will require an —ing affix, whereas if the planning frame is that of a passive sentence (e.g. ‘The girl is being hit by the boy with a flower’) the auxiliary verb will require an —ing affix (but  the main verb will not) and the preposition by will be needed.
The selected affixes and function words are represented within the planning frame in phonological form, and, as we have already noted, the phonological forms of the content words have been retrieved from the lexicon: when all these phonological representations are put together under the control of the planning frame, we have the positional-level representation for the sentence, a level at which a specific sentence is represented in phonological form.
At the fourth level, phonetic rules about English are applied to this positional-level representation in order to create the corresponding phonetic-level representation .
At this level, phonetic details about the intended sentence — e.g. the difference between the pronunciations of the plural -s in cats ,dogs , and churches — are specified.
The final level in Garrett's model —the articulatory-level representation — is generated from the phonetic level.
This level is the articulatory representation of the sentence: a set of instructions from the speech musculature which, when followed, cause the sentence to be spoken.
In Chapter 9 we discuss briefly how this model can be used to interpret various types of disturbance of sentence production seen in people with language disorders due to brain damage; a more detailed account of such interpretations is given by Schwartz (1985).
The model has also been used to explain some aspects of sentence production in normals, such as slips of the tongue and hesitations: for more on this, see Garrett (1982).
Spontaneous speech consists of sequences, not just single sentences, and there are some properties of spontaneous speech which can only be appreciated when one examines language production above the sentence level; hence this is the topic we next discuss.
8.3 Encoding units in spontaneous speech
As we mentioned in the previous chapter (Section 7.1) spontaneous speech and written language have many important differences.
The most striking difference is that whereas formal written language consists of clearly segmented sentences which are normally completely grammatical, spoken language is not clearly divided  into sentences, and typically contains many errors.
These errors range from inadvertent use of an incorrect word to grammatical errors of various kinds.
(As we pointed out in Chapter 1, both kinds of error frequently arise from performance limitations of the speaker rather than from competence limitations.)
Furthermore, spontaneous speech, even in the most articulate speakers, is filled with hesitations (during which speakers say such things as ‘er’ and ‘um’, or repeat a word or syllable) and silences.
This is well illustrated by the following extract, taken from an undergraduate's description of the events depicted in a puppet film.
Since the student is describing a series of very simple events which he has just been watching, it might be expected that he would produce a highly fluent and error-free description with no hesitations.
However, as you can see, this was not the case.
(In order to identify the false starts and repetitions, these have been emboldened.
Short pauses are indicated by a + sign and longer pauses by a ++ sign.)
Well Mrs Toad is having a sale in her shop + + she has laid out her caish + cash register + + an' a number of pots of tea + + it's gonna be a special sale because + + so she has th' + a sign up saying + prices are slashed + so she hopes lots of customers will be coming along + to visit her + + while she ‘s waiting for customers + she goes about setting out the rest of + of the shop + + for things in the sale + + an’ she brings on + large cans of tin + of tea + + for + she can only carry one at a time + so she walks on with one and puts it on the counter + +
Both unfilled pauses and other types of hesitation — false starts, repetitions, filled pauses (in which speakers say things like er and um )— have been analysed in studies of spontaneous speech.
As we will see, hesitations are an important subject of study because of what they can reveal about the mechanisms being employed when people produce spoken language.
8.3.1 Pauses and hesitations in spontaneous speech
Goldman-Eisler (1968) carried out an extensive study of the pause patterns of spontaneous speech.
She found that the proportion of time taken up by pauses varied considerably between subjects, although there was also considerable variation within subjects depending on the topic of conversation.
In interviews of eight subjects the mean proportion of utterance time taken up by pauses varied across the subjects from 4 to 54 per cent.
The variation across topics showed a spread of 13 to 63 per cent.
A similar pattern emerged when subjects were asked to describe picture stories.
The proportion of time spent in silence varied between 16 and 62 per cent, with most subjects pausing between 40 and 50 per cent of their total speaking time.
Pausing thus seems to be an integral part of spontaneous speech.
However, this does not tell us whether pausing is a necessary part of language production.
An ingenious experiment by Beattie and Bradbury (1979) suggests that it is.
This experiment used an operant conditioning technique in order to modify subjects' spontaneous pause patterns while they were telling stories.
A light came on whenever a subject paused for longer than 600 milliseconds, although subjects were merely told that the light indicated when their story-telling was poor.
The presence of the light was sufficient to reduce the number of long pauses by 35 per cent.
However, this reduction was accompanied by a very marked increase in the amount of repetition; subjects began repeating syllables, words and even whole phrases to a much greater extent than they had-done when they were pausing naturally.
By the end of the conditioning procedure the number of such repetitions had increased by 104 per cent.
This result suggests that unfilled pauses are necessary for planning spontaneous speech.
When the amount of pausing drops below what it would naturally be for a given speaker in a particular situation, the number of errors which the speaker makes increases.
So if pauses are necessary, it is legitimate to ask what a speaker is doing during these periods of silence.
The most plausible explanation is that pauses are used by a speaker for planning what to say next.
However, it is perhaps important to point out that there are presumably at least two distinct stages in planning what to say next.
One concerns planning what to say, the other how to say it.
It is not always possible to distinguish these two planning processes in experiments, particularly in considering pauses.
However, it is a distinction which we should keep in mind.
Studies of pause patterns in speech have been carried out since the 1950s.
The early studies (such as those of Lounsbury, 1954 and  Goldman-Eisler, 1958) were based on probabilistic models of language, in which the major concern was the predictability of the next word from the word or words which had gone before.
These studies, therefore, looked for links between pause patterns and the predictability of successive words.
However, as we showed in Chapter 7, there is good evidence that links between successive words are much less important for language processing than the relation of individual words to larger linguistic units, particularly clauses.
More recent studies of pause patterns in spontaneous speech have studied the relationship between pauses and syntactic units.
A study by Beattie (1983) analysed pauses occurring in the speech of five subjects involved in academic discussion.
(This was either a discussion between a teacher and an undergraduate, or between two members of a seminar group.)
Two types of hesitation were considered:unfilled pauses of 200 milliseconds or more; and filled pauses consisting of sounds like er ,ah and um .
Beattie found that over half of both the filled and unfilled pauses occurred immediately before the beginning of a clause.
In addition, there was a strong tendency for the remaining pauses to occur in the first half of a clause rather than the second half.
At first sight, this finding suggests very strongly that the clause is an important unit in the planning of spontaneous speech.
However, the overall pattern which is presented by Beattie's data suggests a rather more complicated picture.
If the clause is the main unit of encoding then we would expect to find that almost all clauses in the speech corpus Beattie analysed should contain pauses, particularly in initial position.
This was not so.
Overall, only just over 30 per cent of clauses had a filled or unfilled pause in clause-initial position.
Before we consider why this was so, it is important to realise that in the kind of discussions which Beattie analysed ‘automatic’ speech is rare.
Discussions of academic topics typically involve a great deal of thought and the production of novel utterances, rather than a series of well-practised utterances like ‘Hello’ and ‘How are you?’.
In other words, we can assume that most of the speech analysed by Beattie was actually being planned as the discussions proceeded.
This planning appeared to have a cyclical character because Beattie's data showed that for each speaker there were both hesitant and fluent phases following one after the other.
The difference between these two phases was particularly noticeable in  the production of long clauses (between 6 and 10 words long).
During a hesitant phase, a speaker would pause more often and for longer periods in the long clauses, than during a fluent phase.
This suggests that during a fluent phase a speaker has already done a major part of the planning before beginning to speak a clause.
Beattie concludes that it is only during the hesitant phase of a speech cycle that the clause is a major unit of speech planning.
However, even in the hesitant phase, the planning of short clauses appears to have occurred during preceding utterances, since over 80 per cent of four-word clauses did not contain a pause in initial position, and nearly 75 per cent of three-word clauses did not contain any pauses at all.
During the fluent phase, planning takes place over a larger unit than the clause.
Beattie suggests that this is because much of the semantic planning for this phase has been carried out during the preceding hesitant phase.
In the corpus analysed by Beattie, the higher order units of planning (which spanned a complete hesitant/fluent cycle) were in the region of 8.8 clauses in length.
These higher order units are probably semantic in nature as Butterworth (1975,1976) has suggested.
However, even though the general semantic content of a series of utterances may have been worked out, detailed semantic and syntactic planning still has to proceed.
Since this may take a considerable time, speakers typically begin to speak before they have completed all the necessary planning for a particular speech cycle.
Planning of the final stages of a cycle is completed during the hesitant phase.
8.3.2 Gaze patterns in spontaneous speech
Further evidence about the amount of cognitive planning being carried out in fluent and hesitant phases of spontaneous speech comes from studies of the gaze patterns of a speaker.
We would expect to find that, if more planning is occurring during hesitant phases, speakers should also look less at their listeners during these phases.
This is because it has been assumed (e.g. by Argyle and Cook, 1976) that monitoring the behaviour of a listener involves a certain amount of attention and often increases the speaker's level  of arousal.
For both reasons, the amount of looking at the listener should go down during a phase of speech production which involves a large amount of cognitive planning.
Kendon (1967) found that speakers do tend to look at listeners more during fluent speech than during hesitant speech.
Speakers looked at listeners 50 per cent of the time while speaking fluently compared with only 20 per cent of the time during hesitant speech.
A similar result was obtained by Beattie (1983) in an analysis of gaze patterns during tutorials at the University of Cambridge.
He found that speakers tended to look most at listeners during the most fluent phases of speech.
Furthermore, if speakers did not avert their eyes during the planning (hesitant) phases in their speech, there was a marked increase in speech disturbances.
There were more false starts, repetitions and parenthetic remarks during hesitant phases in which a speaker looked at a listener and more of these disturbances in the immediately following ‘fluent’ phase.
This suggests that if a planning phase of speech is disrupted, then the following phase which forms part of the same semantic cycle will no longer be fluent because the prior semantic planning necessary for fluent speech has not been completed.
Evidence about the patterns of gaze which speakers normally adopt thus supports the view that the hesitant phase of speech is one in which planning of current and following utterances (within the same cycle) is carried out.
8.4 Errors in spontaneous speech
So far in this chapter we have considered what patterns of gaze and the distribution of pauses can tell us about the kind of units which might be involved in the planning of spontaneous speech.
In this section, we will consider another aspect of speech which can also provide valuable insights into the nature of language production.
This is the study of errors in spoken language.
It is important to point out here that considerable attention has been paid both to errors made by normal speakers and errors made by speakers who have a particular acquired disorder of spoken language.
In this chapter we will be concerned only with errors made by normal speakers.
However, a complete picture of error patterns can only emerge by comparing errors made by normal  speakers with those made by speakers with a language disorder.
We discuss the different kinds of error produced by adults suffering from an acquired disorder of language in Chapter 9.
8.4.1 Errors in the production of items from ‘closed’ and ‘open’classes
One of the questions frequently asked by researchers studying errors in the production of words in spontaneous speech is whether there are differences in the error patterns for different  clashes of word, the most frequently considered being open and closed classes of vocabulary items.
You may remember that we discussed the distinction between open and closed classes in Chapter 3 when we described research into early syntactic development in children's speech.
Words which fall into the open class are those which are potentially unlimited in number because new words can be added to the language as the need arises.
These words —nouns ,verbs ,adjectives and some adverbs — are also referred to as content words because they express the main meaning in an utterance.
Words in the closed class, on the other hand, are used in order to express grammatical relations between words.
Thus, words such as articles (a/the),conjunctions (and/but/because),demonstratives (this/that),pronouns (he/she/it),auxiliary verbs , many adverbs and prepositions (up/down/behind/ between/in, etc.) fall into the closed class.
New words cannot be added to this class — hence its name.
Since words in the closed class have a syntactic rather than a semantic role in utterances they are also known as function words.
In addition to whole words, the closed class also contains affixes .
These are word segments which are added to the beginning or end of a word in order to form a slightly different (but related) word, or to indicate tense or number.
Affixes used to mark tense or number (e.g. he is paint ing or he paint s ) are known as inflectional affixes and those used to derive related words (e.g. paint er or re paint ) are known as derivational affixes .
Since we wish to discuss both words and affixes we will use the more general term lexical item (rather than word ) for the remainder of this chapter.
Like many distinctions, that between open and closed classes of lexical items is not an absolute one.
In particular,prepositions , which are part of the closed class because of the fixed size of their  numbers and their grammatical function, often express an essential aspect of the meaning of an utterance.
However, for the moment we will accept the division of lexical items into open and closed classes.
Stemberger (1984) has carried out a recent study of lexical errors produced by normal adult native speakers of English in natural speech settings.
He divided the errors collected into three categories:substitution errors (where an incorrect lexical item is produced instead of the target);loss errors (where a speaker fails to produce any lexical item); and addition errors (where a speaker produces more lexical items than intended).
Stemberger gives the following examples of errors of each type (φ indicates where a lexical item has been omitted):
substitution:
Oh, you did the wash .
(dishes)
That happened at a couple of other people.
(to)
loss:
I just wanted to φ that.
(to ask that)
You wouldn't have to worry φ that.
(about that)
addition:
I've been keep thinking that.
I don't want to strain ed it.
Stemberger compared the frequencies with which these three types of error occurred with open and closed class lexical items.
He found that open class lexical items were mostly involved in substitution errors, while closed class lexical items were most frequently lost or added.
The most extreme difference occurred between open class items and affixes. 96 per cent of errors in the production of open class items were substitution errors, and only 4 per cent loss or addition errors, whereas for affixes there were only 24 per cent substitution errors but 76 per cent loss and addition errors.
Since addition errors occurred least frequently for both open and closed classes of lexical items, the major type of error occurring with closed class lexical items was loss.
This might suggest that the major difficulty with closed class items is that they are harder to access than open class items.
However, although addition errors occurred with a generally low frequency, they were considerably more frequent for closed than for open class items.
So it seems unlikely that closed class items were more difficult to retrieve than open class items.
Indeed, since closed class items are among the  most frequent lexical items in spoken and written English, it seems extremely unlikely that they should be difficult to retrieve.
Stemberger also investigated phonological errors .
These are errors of pronunciation , as the following two examples illustrate:
I thought I heard a thollow hud .
(hollow thud)
But shells don't go shoft do they?(soft)
Stemberger found that the great majority of phonological errors occurred with open class lexical items.
In fact, phonological errors were nine times more frequent with open class items than with closed class items, even though there were more closed than open class items in Stemberger's corpus.
This difference persisted even when potentially confounding factors such as stress and syllable position were taken into account.
These findings suggest that, for speech production, there are two different processing vocabularies, one for open class items and the other for closed class items.
Production of lexical items from each of these two classes has its own distinct error pattern; production of words from the open class is characterised by a high frequency of substitution and phonological errors, while production of words from the closed class is characterised by a high frequency of loss and addition errors and very few phonological errors.
In addition, Stemberger notes that lexical items from the closed class are less likely to be involved in sequencing errors (in which lexical items occur in the wrong order in an utterance) and in shift errors (in which an item appears either too early or too late in an utterance).
An example of each of these kinds of error is shown below:
sequencing error: You just count wheels on a light .
(intended: You just count lights on a wheel)
shift error: We tried it making — making it with gravy.
We have just suggested that one way of interpreting these apparently different patterns of error in the production of open and closed class lexical items is to argue for the existence of two separate vocabularies which are accessed during the production of spoken language.
This view has been put forward by Garrett (1980; 1982).
However, Stemberger (1984) has recently argued that the grounds for treating open and closed class lexical items as coming from two  separate vocabularies are not convincing since many of the differences in error patterns for the two classes can be explained by differences in the frequency of items in the two classes, and by differences in the potential frequency with which certain kinds of error can occur for items in the two classes.
This issue is also relevant to Chapter 9, in which we discuss the pattern of lexical errors in patients with acquired disorders of the production of spoken language.
8.4.2 The Full Listing Hypothesis
Another question which has been addressed by studies of lexical errors in speech production is whether every word-form which a speaker knows is explicitly listed in that speaker's mental lexicon.
This is known as the Full Listing Hypothesis .
An alternative to having a separate entry for every word-form is that only base forms are listed in the lexicon, together with a set of lexical rules for deriving all regularly inflected forms of the base item, and a listing of all irregularly inflected forms.
According to this view, there would be an entry for a base form like sing , but not for the present participle singing , or the 3rd person singular present sings .
Instead, there would be a set of lexical rules indicating which affix had to be added to produce each inflected form.
However, an irregular inflection like sung would be listed, so that the lexical rule for the formation of the regular plural present tense would not be inappropriately applied to the base to give singed .
We will call this the Stem + Lexical Rules Hypothesis .
(It has also been suggested that derivations from a stem might not be listed explicitly, so that there would be no separate listing of a word like singer , only a listing of the lexical rule for the formation of an agentive.)
The plausibility of the Full Listing Hypothesis versus that of the Stem + Lexical Rules Hypothesis has been investigated by looking at errors in speech production.
These often show a dissociation between the stem of a lexical item and its affix.
This is particularly striking in the case of inflections .
Consider the following example of a sequencing error taken from Stemberger (1984) which we considered earlier:
You just count wheels on a light .
Note that the inflection occurring with the transposed lexical item has remained in its grammatically correct position, rather than moving with the base item to which it would normally have been attached.
(If the s inflection had moved with its base, the speaker would have said: You just count wheel on a light s .)
There are many other examples of dissociation between base and inflection, and several other examples from Stemberger's corpus which we quoted earlier illustrate this.
However, in order to accept that such examples are evidence in favour of a Stem + Lexical Rules Hypothesis of entries in a mental lexicon, it is necessary to show that similar patterns of dissociation do not occur for other word-final fragments.
As Butterworth (1983) points out, there are cases where similar exchanges do occur for other word-final fragments.
For example, Garrett (1980) gives the following examples which involve transposition of derivational affixes:
square it face ly (face it squarely)
I've got a load of cook en chick ed (…chicken cooked)
Fromkin (1973) gives the example of a speaker erroneously producing fran sanisko for San Francisco, in which the final segment of Francisco is produced in its correct position but the preceding segment is not.
Here what is transposed is not an affix at all, but just a syllable.
In order to accept that the data from speech errors are contrary to the Full Listing Hypothesis, it is necessary to show that the fate of inflections is different from that of other word-final fragments.
As we have just seen, this is not the case.
So, as Butterworth (1983) concludes, the data derived from the speech errors of normal speakers is not decisive in rejecting the Full Listing Hypothesis.
However, as with our discussion of open and closed lexical classes, the evidence from patients with acquired disorders of spoken and written language is also highly relevant.
Such patients are discussed in Chapter 9.
8.5 Formulating sentences
As we mentioned in Chapter 7, it is probably inappropriate to think of normal spontaneous speech as consisting of sentences .
However, since much of the research which we want to discuss in the following section is concerned with the formulation of sentences, we will use this term in order to avoid confusion.
Since the arguments we wish to make are not dependent on whether speakers actually produce sentences, this simplification will not affect our discussion.
Our principle concern in this section of the chapter will be factors which affect the kind of syntactic form which speakers use on a particular occasion.
We will discuss two issues, the existence of various alternative ways of expressing essentially similar information, and the relation between the accessibility of particular lexical items and the production of particular syntactic forms.
8.5.1 Syntactic paraphrases
One question which has concerned psycholinguists for many years is why there exist in English (and in other languages) several alternative syntactic options (or paraphrases) for expressing essentially the same information.
For example, the following three sentences all express the same basic meaning:
(a)
The dog chased the tabby cat
(b)
The tabby cat was chased by the dog
(c)
The cat the dog chased was a tabby
It was an implicit assumption of much of the early research carried out in psycholinguistics in the 1960s (see Greene, 1972 for a review) that certain syntactic forms were exactly equivalent in meaning.
This assumption was necessary for the commonly held view that complex syntactic forms (like negatives, passives and interrogatives) might be constructed by applying transformational rules to simpler syntactic forms.
According to this view (e.g. Miller and McKean, 1964), passive sentences were constructed by the application of a passive transformation rule to an active, and negative sentences by application of a negative transformation rule to an affirmative.
It was further assumed that passive negative sentences were constructed by the application of two transformational rules (passive and negative).
However subsequent research has shown that both negatives and  passives are used in response to the presence of particular pragmatic factors.
In the case of negatives, these factors are different from those associated with use of affirmatives, and in the case of passives these factors are different from those associated with the use of actives.
The use of negative forms was investigated by Wason (1965) and Greene (1970a; b) who showed that the natural function of a negative is to signal denial or contradiction of a prior assertion.
Comprehension of negative sentences is facilitated when negation is being used to fulfil one of these functions.
For example, in Greene's (1970a) experiment subjects were asked to sort pairs of statements according to whether the statements in a pair had the same or a different meaning.
She argued that if the second statement in a pair contained a negative then it would be more natural for the meaning of the two statements to be different , whereas if the two statements were both affirmative it would be more natural for the two statements to have the same meaning.
Greene found subjects' performance in her sorting task supported this claim.
For statement pairs containing a negative, sorting was faster when the two statements meant something different than it was when the two statements meant the same.
Thus, it took subjects less time to decide that the following sentences had a different meaning:
x exceeds y
x does not exceed y
than to decide that the following two sentences had the same meaning:
y exceeds x
x does not exceed y.
Pairs of active and passive sentences were included as controls and performance with these showed the opposite pattern.
A different meaning pair like:
x exceeds y
x is exceeded by y
took longer to sort than a same meaning pair like:
y exceeds x
x is exceeded by y.
Unfortunately for our present discussion, Greene did not investigate the production of negatives, only comprehension.
However, it seems reasonable to assume that pragmatic factors in comprehension will also be present in production.
Direct evidence about pragmatic factors in production comes from studies of the passive.
Turner and Rommetveit (1967) and Tannenbaum and Williams (1968) manipulated subjects' focus of attention while they were describing events depicted in simple line drawings.
They predicted that when a subject's attention was on the actor a situation would be described using an active , whereas when attention was on the acted-upon (the recipient of action), greater use would be made of the passive.
In the experiment carried out by Tannenbaum and Williams, focus of attention was manipulated by prior presentation of a preamble concerning either the acted or acted-upon depicted in the line drawing.
Subjects were told whether to produce an active or a passive sentence to describe the picture, and production latency was measured.
The prediction was that when a subject's attention had been focused on the acted-upon it would be easier (and so take less time) to produce a passive, whereas an active would be easier when the preamble concerned the actor.
This was confirmed by the pattern of response latencies; there was a significant interaction between prior focus and sentence voice.
In the Turner and Rommetveit experiment (which was carried out with children) attentional focus was manipulated either by asking subjects a question about actor or acted-upon, or by presenting one part of the line drawing before revealing the whole drawing.
In the latter case either the actor or the acted-upon was revealed first.
Both techniques were successful in manipulating the way in which the line drawing was described.
When the acted-upon was presented first, or emphasised through questioning, significantly more passives were produced than when the actor received prior attentional focus.
Subsequent experiments have revealed that the concept of  attentional focus can be broken down into two separate factors: one concerns the intrinsic importance of actor and acted-upon; the other concerns informational aspects of actor and acted-upon.
In the case of intrinsic importance, the factor that has been most extensively studied is animacy .
Experiments by Prentice, Barrett and Semmel (1966), Harris (1977; 1978) and Dewart (1979) have shown that the passive is likely to be used when the acted-upon is more animate than the actor.
For example, a collision between a pedestrian and a car is very likely to be described by a speaker as a boy being run over by a car , rather than as a car knocking a boy down .
This pattern emerged clearly in the experiment by Harris (1978) in which both adults and children were asked to describe pictures in which the animacy of actor and acted-upon was systematically varied.
Actor and acted-upon were either human (e.g., a girl, a milkman), animal (e.g. a cow, a horse) or inanimate (e.g. a bicycle, a ball) and the depicted action involved either an actor and acted-upon of the same class (e.g. two humans) or a different class (e.g. a human and an animal).
The number of passives produced in response to the different pictures varied considerably.
As predicted, most passives were produced when the acted-upon was more animate than the actor and least when the actor was more animate than the acted-upon.
The difference between these conditions was striking: 154 passives were produced when the acted-upon was more animate and only 3 when the actor was more animate.
17 passives were produced when actor and acted-upon came from the same class.
Animacy is, however, only one of the factors which influences selection of a passive form to describe a particular situation.
In most situations, a particular sentence will form part of a larger discourse and, when this is so, another important factor affecting use of the passive is whether actor or acted-upon is the theme of a discourse (i.e. what the discourse is about).
When an acted-upon, rather than an actor, is thematic, use of the passive increases.
This was demonstrated by an experiment reported in Harris (1977) in which both animacy and discourse theme were manipulated.
Subjects (children aged between 4:6 and 9:9) were presented with sequences of pictures.
At the beginning of each sequence, a picture of a human, an animal or object was presented, and the sequence was introduced as being a ‘story’ about whoever or whatever was depicted in this initial picture.
Each picture in the  sequence showed the thematic person, animal or object in interaction with other people, animals and objects.
For example, in one sequence the following events (all involving a girl) were depicted:
a girl following a boy
a girl stroking a rabbit
a girl cleaning a car
a soldier carrying a girl
a dog licking a girl
a bus knocking over a girl
The results of this experiment showed that both the animacy of the actor and acted-upon, and whether actor or acted-upon was thematic, affected subjects' use of passives.
When the acted-upon was more animate than the actor 133 passives were used, compared with only 16 when this pattern was reversed.
When the acted-upon was thematic, 155 passives were produced, but when the actor was thematic, only 44 were produced.
The effects of animacy and thematisation were additive: the greatest number of passives (96) occurred when the acted-upon was both thematic and more animate than the actor, and the least (only 1) when the actor was thematic and more animate than the acted-upon.
One way of viewing these results is to suggest that what determines use of the passive rather than the active is selection of the acted-upon as the first noun to be mentioned.
This is because there is considerable evidence that it is normal in English to put more animate nouns and thematic information in sentence-initial position.
(See Halliday, 1967 and Harris, 1977 for a discussion of this view.)
We will therefore broaden our discussion of factors involved in the formulation of sentences to consider evidence concerning the accessibility of lexical items.
8.5.2 Lexical accessibility and syntactic structure
The possible role of lexical accessibility in determining the syntax of a speaker's utterances has been considered at length by Bock (1982).
She argues that there are good reasons for supposing that variables such as frequency, concreteness and prior priming — all of  which influence the speed with which individual words can be accessed (see Chapter 6)— will affect sentence production.
More specifically, Bock argues, syntactic alternatives are employed in order to place lexical information, that has already been retrieved, earlier in an utterance than lexical information which has not been retrieved.
The reasons for adopting this view that syntactic production and the accessibility of individual lexical items are closely related are twofold.
First, there are a priori grounds for supposing that it would be inefficient to have a production system in which words are accessed before the constituent in which they are to appear.
We can assume that the processing resources available for language production are limited, and attempts to hold items for future as well as current constituents would result in an unnecessary drain on processing resources.
However, since access to lexical items appears to be at least partly an automatic process in which words are accessed as a result of unplanned factors (such as prior mention by the present speaker or an earlier speaker), it seems plausible to suggest that the production system should be able to adapt in order to incorporate automatically accessed lexical items into current constituents.
Hence the existence of syntactic alternatives which allow word order to be varied.
The second reason for Bock's claim about relations between syntactic form and lexical accessibility in production is an empirical one.
This is the finding that the same kinds of factors responsible for increasing lexical accessibility of single words (mainly in studies of word recognition, but also in some studies of production) are also responsible for affecting word order in sentences .
So, for example, in the case of the passive both animacy and prior access of a particular word (occurring when a noun is thematic) are factors which have been shown to influence single word identification.
This was illustrated by Glanzer and Koppenaal (1977), who showed that animate words are more likely to be recalled from a list than inanimate words; and many experiments have established the existence of priming effects in word recognition (see Chapter 6).
It would, however, be incorrect to assume that syntactic aspects of language production could only be influenced by automatic accessing of particular lexical items.
It is also likely that in some cases syntax is influenced by the controlled access of particular lexical items — the desire to convey certain items of information  earlier, rather than later, in an utterance.
On yet other occasions the access of lexical items will be constrained by the selection of particular syntactic options which are required in order to convey particular communicative intentions.
Thus, it would seem that, like other aspects of language processing we have discussed in this book, sentence production appears to have both top-down and bottom-up components.
We need to bear this in mind in the next chapter when we consider some of the problems of lexical retrieval experienced by patients exhibiting an acquired language disorder.
8.6 Overview
The production of written or spoken language at the single-word level (as, for example, when you answer a question like ‘What is the large grey animal with tusks and a trunk?’ or write down the name of a picture shown to you) is not difficult to explain.
It involves retrieval of single items from a lexical output system of the kind described in Chapter 6.
Language production, however, is normally the production of syntactically structured sequences of words.
We described Garrett's ideas about the various processes required for the production of spoken sentences, and the various factors (discourse theme, animacy, lexical accessibility) which determine the choice a speaker makes between different sentences expressing the same meaning — passives versus actives, for example.
In practice, spontaneous speech does not consist of fluent sequences of grammatically correct sentences, but is hesitant, sometimes repetitive and grammatically incorrect.
There is good evidence that these imperfections are imposed by performance limitations.
The speaker's ability to plan utterances is not sufficiently good to allow planning and production to co-occur smoothly and without interference; and so there are periods when production must suffer if planning is to proceed.
Hence errors at a variety of levels (from the interchanging of elements of adjacent words up to the abandoning of whole sentences) are inevitable.
Acquired disorders of language
9.1 Introduction
In Chapter 4, we discussed developmental disorders of language: developmental dysphasia, developmental dyslexia, and developmental dysgraphia.
The term ‘developmental’ here indicates that there has been a failure to acquire the relevant linguistic skill (the use of spoken language, reading or writing) at the normal rate, during the course of development.
There is another way in which language disorder can occur, however.
In people who had achieved entirely normal linguistic skills, brain damage can reduce or even abolish such skills.
Here the term ‘acquired disorder of language’ is used; and this is the topic of this chapter.
The chapter has two aims.
The first is to describe the various forms that acquired language disorders can take — that is, the various different ways in which language processing can be impaired by various forms of brain damage.
The second aim is to discuss these patterns of language impairment in relation to the models of normal language processing we have described in previous chapters.
If one can demonstrate clear relationships between specific patterns of impairment and specific models of normal processing, then, one can use the patterns of impairment as evidence for or against the models, and one can use the models to explain how the patterns of impairment come about — why patients with language disorders behave in certain ways and not in other ways.
Neurologists of the nineteenth century, such as Broca and Dax, were able to show that damage to the brain often leads to very obvious disturbances in various aspects of cognition.
This is  particularly the case for damage to the left hemisphere of the brain; and this led to the concept of cerebral dominance: the idea that the left hemisphere dominated the right as far as their contributions to cognitive processing were concerned.
It was soon realised, however, that for certain mental processes — visuospatial processing and visual object recognition, for example— it was the right hemisphere of the brain which was the important one.
For all aspects of specifically linguistic processing, however, it remained the case that, in almost all people, the left hemisphere was the important one; and so in people with acquired disorders of language it was almost always the case that the left hemisphere had been damaged.
The concept of an acquired disorder of cognition is, of course, much broader than the concept of an acquired disorder of language, but the latter concept is still extremely general; and neurologists in the second half of the nineteenth century made it more specific in a variety of ways.
It was recognised, for example, that in some people with an acquired disorder of language, it was only the production of language that was impaired, with language perception and comprehension still normal, whilst the opposite pattern (impaired perception or comprehension with intact production) could be seen in other patients.
It was also recognised that there were patients whose language disorder affected only the processing of written language (reading and writing) and others where it was only the processing of spoken language that was impaired.
Further refinements developed: for example, Dejerine (1982) described a patient whose disorder in the processing of written language was confined to its perception — that is , this patient could still write normally, even though his reading had been severely impaired by brain damage.
It had thus become clear by the turn of the century that acquired language disorder was by no means a single entity: instead, a variety of different patterns of disorder could be observed.
Hence a set of terms emerged for describing these different patterns, and this set of terms is still in use today.
Aphasia or acquired dysphasia is used to refer to any acquired disorder in the ability to process spoken language.
If one wishes to distinguish between an input defect (impairment in the perception or comprehension of spoken language) and an output defect (impairment in the production of spoken language), then the terms  receptive aphasia versus expressive aphasia can be used.
Alexia or acquired dyslexia is used to refer to any acquired disorder of reading, and agraphia or acquired dysgraphia to refer to any acquired disorder of writing or spelling.
Anomia (or, occasionally,dysnomia ) is used to refer to an impairment in the ability to produce the right single word when, for example, trying to name an object or to produce a specific word in spontaneous speech.
This is also often referred to simply as ‘word-finding difficulty’.
We have already illustrated ways in which the research on the effects of brain damage on mental processes had taken the form of defining increasingly more specific patterns of disorder.
The general idea that cognitive processing can be impaired was made more specific by showing that particular domains of cognition, such as language, can be selectively impaired with other domains such as memory or attention intact.
This concept of a general language impairment was, in turn, made more specific with the idea that particular sub-abilities (e.g. the production of spoken language) can be impaired whilst other sub-abilities involved in language (e.g. speech perception) are spared.
It should be clear how such findings are of relevance to theories about normal language processing.
If a theory of normal language processing were proposed in which there were a single processing system responsible for dealing with spoken language — a system used both for perceiving and for producing speech — then one ought never to see patients with intact speech perception and impaired speech production, nor patients with the opposite disorder.
The fact that both these forms of aphasia are frequently observed suggests that there are separate systems for perceiving and producing speech; and we have already seen in Chapter 6 that research on normal subjects suggests the same conclusion.
More examples of arguments along these lines are given later in this chapter.
A patient with a language disorder caused by brain damage will normally exhibit various different symptoms of the disorder, not just a single symptom.
A collection of symptoms is called a syndrome .
Hence the neurologists of the late nineteenth and early twentieth centuries, having recognised that various different patterns of language disorder existed, went on to attempt to define these as syndromes — for example, to define several different categories of aphasia, each category being defined as a list of the  symptoms that are seen in patients belonging to that category.
There are two very different uses to which the concept of syndrome can be put, and it is important to appreciate the distinction between these uses which we will refer to as the theoretical and the clinical .
The theoretical use is as follows: if there exists a syndrome such that every patient who exhibits symptom x also exhibits symptoms y and z, it would seem reasonable to try to explain this by developing a theory in which there exists a single processing system whose properties are such that, if the system were malfunctioning, all three symptoms x, y and z would automatically occur.
One starts off with a syndrome and uses it to develop a theory of normal processing.
The clinical use is different: here one uses syndromes to help one be aware that, for example, if you notice symptom x in a patient, you ought to test for the presence of symptoms y and z, since they are very likely to be present.
The difference between these two uses of syndromes becomes clear if one considers the following situation: suppose, after years of finding that, every time a patient exhibits symptom x, symptoms y and z are also exhibited, a patient comes along who has symptom x but not symptom y.
This makes no difference as far as the clinical use of the concept of syndrome is concerned: it remains true that if you see x it is a good idea to look for y and z (since they are so likely to be present).
But the single patient with x and not y does make a great deal of difference as far as the theoretical use of the concept is concerned.
The pattern of symptoms in this patient refutes the theory that the reason why x, y and z co-occur is that they all reflect damage to a single system (since if that theory were true you could never get x without y); indeed, this patient shows that there actually is no syndrome (in the theoretical sense) defined by the joint occurrence of all three symptoms.
We have discussed this point because we are about to describe various syndromes of aphasia, and it is important to realise that one can think of each syndrome in the theoretical sense or in the clinical sense.
We will return to this point later.
9.2 Syndromes of aphasia
It is perhaps not surprising that two of the major varieties of  aphasia are named after two nineteenth-century neurologists, Broca and Wernicke.
We will begin by describing Broca's aphasia and Wernicke's aphasia, and then go on to describe other frequently seen forms of aphasia.
9.2.1 Broca's aphasia
Figure 15 depicts a fairly everyday household scene.
Before you read on, write down a description, about half a page in length, of what is going on in the scene.
FIGURE 15 The assessment of aphasia and related disorders
Here is a spoken description of what is going on in Figure 15 given by a patient with Broca's aphasia (from Funnell, 1983):
(Points to the water on the floor and laughs)‘Ah…ah…girl and boy, ah oh er er dear…girl (points to the woman) cof (points to the cloth) and, er oh er dear me…er (points to the water) um steps (points to the stool) er steps um window, curtains…a pot and an er (points to the water) oh dear me…
OK.’
The nature of this description is even clearer if one deletes the pauses, interjections, and the ums and ahs; the patient describes the picture thus: ‘Girl and boy, girl, cloth and steps, steps, window, curtains, a pot and an…’
This kind of spoken language production is characteristic of Broca's aphasia.
Its basic features are:
(a)
speech is effortful, extremely hesitant, contains long pauses and is very non-fluent;
(b)
virtually no grammatical structure is evident — certainly no proper sentences, and a few or no function words, prefixes or suffixes;
(c)
what is produced consists largely of specific nouns, and verbs are notably under-represented;
(d)
even those words which can be produced are often poorly articulated;
(e)
the comprehension of speech, at least for single words and simple sentences, is often very good despite the very severe deficit in the production of speech.
9.2.2 Wernicke's aphasia
The pattern of symptoms exhibited in Wernicke's aphasia is in many respects exactly the opposite of that shown in Broca's aphasia.
Here is how a patient with Wernicke's aphasia gave an account of what is going on in Figure 15 (Funnell, 1983):
‘Well it's a it's a it's a place and it's a g-girl and a boy…and the-they've got obviously something which is is made some made made made well it's just beginning to go and be rather unpleasant (ha! ha!) um and this is in the this is the the woman and she's put putting some stuff and the it's it's that's being really too big t-to do and nobody seems to have got anything there at all at all and er it's…
I'm rather surprised that but there you are this this er this stuff this is coming they were both being one and another er put here and er um um I suppose the idea is that the er two people should be fairly good but I think it's going somewhere and as I say it's down again…let's see what else has gone er the the this is just I don't know how she di’ how they did this but it must have been fairly hard when they did it and er I think there isn't v-very much there I think.’
The basic features of Wernicke's aphasia are:
(a)
speech is effortless, fluent and rapid;
(b)
what is said is generally grammatically complex and well-structured, with an abundance of function words, prefixes and suffixes;
(c)
those content words which are produced are very rarely words with specific meanings such as knife ,hair ,dig or spill .
Instead, the content words are mostly very general nouns such as place ,something ,stuff and very general verbs such as got ,put or did ;
(d)
the comprehension of speech is severely impaired; sometimes even single spoken words cannot be understood even though they are correctly perceived.
9.2.3 Conduction aphasia
In this syndrome, the central deficit is in the ability to repeat back what has been heard.
This ability is impaired far more than other aspects of the patient's language.
In severe cases, even the ability to repeat single words is not intact; for example, a patient, KC, was not perfect even when single common concrete nouns were spoken to him, with his task being to repeat back each one immediately.
The kinds of errors he made are illustrated by these responses:
‘face’—‘pace, no, p, p…’
‘woman’—‘woolden, wooden, no…’
‘girl’—‘curn, cur…no’
The failures in repetition are not caused by difficulties in hearing the words themselves.
We know this, firstly, because KC was perfect at understanding the meanings of words spoken to him (as tested by word-to-picture matching) and, secondly, because KC always knew whether his attempt at a repetition was right or  wrong.
If KC repeated ‘girl’ as ‘curn’because that is how he heard it, he would not know his response was wrong.
In cases of conduction aphasia, comprehension of spoken words and simple spoken sentences can be intact.
Speech is usually fluent and grammatical, sometimes with occasional mispronunciations of individual speech sounds and sometimes with word-finding difficulties.
Here is a sample from conversation with two psychologists, the topic of conversation being the linguistic difficulties — particularly, the intrusions of completely unwanted and irrelevant words — KC experienced in the first few weeks after his stroke.
KC ‘What worried me most er at the beginning and now the treat…and now fading to a great degree, was the extraordinary presence of another absence er of another kind of speaking.
Absolutely real.
I mean a real belief that I have the word ‘tiger’ or ‘leopard’or whatever.
An absolute knowledge for it, and I see the picture of it, and it's quite wrong.
It has no connection with this.’
MC ‘Where do the words come from, do you think?’
KC ‘God knows!
But I actually know that I, I, I…li, line, thinking er er myself.
I would think of ‘tiger’ or what not, that it might mean, and this thing has no trans…has no relation to, to, to, to, to, the, the, the sounds that I was looking…’
MC ‘To the word you are looking at?’
KC ‘Yes, yes.
I was imagining a totally different one.
And this was, for a time, for about er three, three, three or four…weeks was still quite strong.’
KC's conversational speech may not be entirely normal — but it is remarkable in comparison to his extremely impaired ability to repeat back single words.
You might be curious as to why this disorder of repetition is called conduction aphasia.
The answer is that the earliest attempt to explain the disorder was that the brain regions for understanding and for producing speech were intact, and what was damaged was the neural pathway which conducted information between these two pathways.
The consequence of this damage was, on this theory, that when the patient heard speech he could understand it but  could not transmit what he had heard to the brain region for speech production, and so could not repeat it.
There are all kinds of reasons why this explanation will not work; we mention it only to explain the origin of the term ‘conduction aphasia’.
9.2.4 Transcortical aphasia
Just as Wernicke's aphasia is in some sense the opposite of Broca's aphasia, so transcortical aphasia is in some sense the opposite of conduction aphasia, because in transcortical aphasia it is repetition which is the best -preserved of the patient's linguistic abilities, with the patient being extremely poor at understanding speech.
In some such patients, speech production is not eliminated, but what is said is usually entirely meaningless and incomprehensible (this syndrome is known as transcortical sensory aphasia).
In others, there is no spontaneous or elicited speech output (this is known as transcortical motor aphasia).
In both syndromes, however, repetition is remarkably preserved.
A severe case of transcortical aphasia is described in detail by Whitaker (1976).
This patient, HCEM, had no spontaneous speech, could not write and could not name objects.
She had some ability to read, but almost no comprehension of spoken or written words.
Her ability to repeat what she heard was the only language task which she could perform at all well.
She could repeat sentences such as‘The window was broken by John’.
Remarkably, when she was given ungrammatical sentences to repeat, she often produced a grammatically correct version: she repeated ‘She write she mother a letter’ as ‘She wrote her mother a letter’, and ‘Do you want to go movies?’as ‘Do you want to go to the movies?’.
In contrast, she never corrected semantic anomalies when repeating (for example, she faithfully repeated such anomalous sentences as ‘I put on your clothes in the morning’ or ‘Eat your milk’).
Why is this called transcortical aphasia?
Again, the term comes from the earliest attempts at explaining the disorder.
The idea was that since repetition is so good the pathways across the cortex (hence trans cortical) from the brain regions for perceiving speech to the brain regions for producing speech are intact.
However, these regions are cut off from the rest of the brain (hence, roughly speaking, disconnecting speech from thought).
Again, we mention  this explanation not to advocate it, but to explain the origin of the term.
9.2.5 Anomic aphasia
As we have seen, word-finding difficulty is common in various kinds of aphasia.
Sometimes it is this which is the patient's most obvious symptom: comprehension of speech is very good and spontaneous speech can be well-articulated, grammatical and fluent, except that every so often the patient stops speaking because he simply cannot find the word he wants to say.
Approximate synonyms, or else circumlocutions, are chosen to fill the gap.
This is anomic aphasia.
9.2.6 Global aphasia
Global aphasia could be thought of as the most extreme form of aphasia.
In patients who suffer this form of aphasia no language ability is spared: speech comprehension, spontaneous speech, repetition (and reading and writing) are all absent or minimal.
9.3 The theoretical interpretation of aphasic syndromes
What do these syndromes tell us about the language-processing system as it exists in intact brains?
Can we use theories of the normal language-processing system to explain the various patterns of abnormality of language behaviour seen in different kinds of aphasic syndromes?
The answer to such questions depends in the first place upon what we mean by ‘syndrome’.
Let us use Broca's aphasia to illustrate this point.
Earlier in this chapter, we distinguished between the syndrome as a theoretical entity and the syndrome as a clinical observation.
In the first case, a syndrome is a set of symptoms which inevitably co-occur, the reason being that a single underlying malfunction of the language-processing system is responsible for all the symptoms.
In the second case, a syndrome is a set of symptoms which frequently (but not necessarily invariably) co-occur; if there are occasional instances (no matter how rare) where some of the symptoms are present and others absent, then one cannot explain the syndrome as due to a single underlying defect generating all the symptoms.
Various attempts have been made to regard Broca's aphasia as a syndrome in the first of these senses — that is, to argue that there is a single underlying deficit in the language processing system which is responsible for all the symptoms of Broca's aphasia.
Different people have had different views on what this single deficit might be: we will mention three such views.
On one interpretation, the form of brain damage suffered in Broca's aphasia has simply made the production of spoken words very effortful.
So the patient says very little and is very non-fluent.
Why is it that content words tend to be preserved and function words omitted?
Because the patient is trying to communicate in the fewest possible words, and a sentence missing its function words is easier to understand than a sentence missing its content words.
One of the many major problems for this idea is that Broca's aphasics usually show better performance with content than with function words even when the task is reading aloud single words: the patient given tie ,and ,the and ant to read aloud might well succeed with tie and ant but not with and or the .
There is no reason (as there might be with spontaneous speech) why the patient should choose to utter the isolated content words here and not the isolated function words.
Another single-deficit interpretation of Broca's aphasia is that it is a phonological deficit (Kean, 1977): an impairment of the aspects of the language-processing system that relate to phonology (the sounds of words, as distinct from their meanings or spellings).
Kean has proposed that what is omitted in spontaneous speech are the elements of sentences which are unstressed — commonly, prefixes, suffixes and function words.
However, this analysis also fails to explain why even the reading aloud of single function words (where there is no question of stress patterns in a sentence) is so often impaired in Broca's aphasia.
Perhaps the most widely held single-deficit theory of Broca's aphasia is that it is a syntactic deficit.
The patient's ability to employ syntactic knowledge is impaired — so spontaneous speech is grammatically very simple, or even ungrammatical, and words which are primarily important in establishing grammatical structure  (function words) are especially affected.
The trouble with all of these theories of Broca's aphasia, and others like them, is that they assume that a single deficit is involved, i.e. that the syndrome is a ‘theoretical syndrome’; and it is becoming increasingly clear that this is not so.
It is perfectly correct that, if you compare a group of Broca's aphasics with a control group and measure (a) rate of spontaneous speech;(b) number of function words used;(c) number of prefixes and suffixes used;(d) number of verbs used;(e) complexity of grammatical structure; and (f) accuracy of pronunciation, you will find that the Broca's aphasics have a lower mean than the control group on all six measures.
What you will not find, however, is that every individual Broca's patient is lower on all six measures (see, e.g. Berndt, 1985).
Some patients classified as Broca's aphasics have dysfluent speech but normal complexity of grammatical structure; others have reduced occurrence of function words, prefixes and suffixes but fluent speech; and so on.
In general, an individual Broca's aphasic is likely to show only some subset of the six symptoms listed above.
Thus, it seems that it is misguided even to try to find a single deficit theory of Broca's aphasia, because Broca's aphasia is a clinical syndrome, not a theoretical syndrome.
Most patients with effortful, poorly articulated speech will have ungrammatical speech output with good speech comprehension, but not all — any two of these symptoms can exist without the third, so one cannot offer a single theory accounting for all three  symptoms at once.
Although we have illustrated this point with reference to Broca's aphasia, it is generally true for all the aphasic syndromes.
All are clinical syndromes, not theoretical syndromes.
They are of value in guiding the clinician's testing of aphasic patients, but are not of direct value in relation to theorising about the language-processing system and how it can go wrong after brain damage, because one cannot answer such question as ‘In Broca's aphasia, how is the language-processing system malfunctioning?’
This does not at all mean that the study of aphasia and theories about the language processing system have nothing to say to each other.
On the contrary, investigation into relationships between these two approaches to language is currently flourishing.
This kind of research does not depend upon the concept of a syndrome, however.
Instead of generalising findings from one patient to others  by treating them all as representing instances of a single syndrome, the approach is to treat them all as people in whom a language-processing system which, before their neurological damage, was the same for all of them, has been impaired in some specific way.
The nature of the impairment of the system might be different for every patient, so each patient is unique; yet all the impairments shown by all the patients are interpreted with reference to a single theory of the language processing system.
How this approach works will become clearer when we come to discuss acquired disorders of reading and spelling later in the chapter; but we will give one example relating to disorders of spoken language — specifically, to contrasting patterns of disordered sentence production.
Schwartz (1985) has discussed ways in which the model of normal sentence production proposed by Garrett (1980, 1981, 1982), which we described in Chapter 8, might be used to interpret defects of sentence production seen in patients with any form of expressive aphasia.
Consider the example from Funnell (1983) which we gave earlier in this chapter, of a patient with Broca's aphasia attempting to describe the scene shown in Figure 15.
This patient's verbal output consisted almost entirely of content words there were almost no function words and no grammatical structure.
This pattern can be described economically in terms of Garrett's model if we propose that, of the two processes applied to the message-level representation in order to create the functional-level representation, one has been completely disabled, with the other remaining almost intact.
The nearly intact process is that which selects major lexical items (content words) from the lexicon: hence the patient produces appropriate content words (though we cannot explain why verbs are poorly produced relative to nouns).
The abolished process is that which generates the functional predicate/argument structure for a sentence.
If this structure is not generated the patient will be left with a collection of content words but no argument structure in which to insert them.
Hence the eventual output can only be an unstructured sequence of content words.
Other symptoms frequently seen in the sentence production of Broca's aphasics would need to be explained as due to additional deficits in the sentence-production system (e.g. if there is a specific problem with prefixes and suffixes, that would be seen as arising during attempts to construct the positional-level representation).
The contrasting pattern characteristic of Wernicke's aphasia and illustrated by the second of the patients of Funnell (1983)— whose attempt at a description of Figure 15 we also quoted earlier in this chapter — could be interpreted, in terms of Garrett's model, as arising when there is a defect of the process which selects the required major lexical items from the lexicon.
However, the process which creates the functional predicate/argument structure remains relatively intact.
Hence when the Wernicke's aphasic is seeking to create a functional-level representation by applying these two processes to the message-level representation, the patient will be left with an appropriate argument structure but no content words to insert into it.
If this structure gives any clues as to what kinds of words the missing ones are — if one can deduce from the structure that one missing word is a verb of action, another is an animate noun, and so on— the patient can insert very general content words by using such deductions (words like did ,person ,thing ).
Thus one would expect to see grammatically structured and grammatically correct sentence production with the specific content words needed replaced by extremely general ones: as is evident in the productions of Funnell's patient.
We refer the interested reader to Schwartz (1985) for more illustrations of ways of applying Garrett's model to aphasic sentence production, since we do not have space here to consider this work further: we must now turn from considering disorders of spoken language to a discussion of disorders of written language reading disorders (acquired dyslexias) and writing disorders (acquired dysgraphias).
9.4 Acquired dyslexias and their relationships to a model of normal reading
In Chapter 6 we set out a dual-route model of reading aloud, distinguishing between a lexical procedure and a non-lexical procedure for converting print to speech.
The lexical procedure works by retrieving previously learned information about the specific letter string being read (and so cannot be used for reading non-words aloud, since the reader has never seen these before).
The non-lexical procedure works by applying rules specifying how a particular letter or letter group is to be pronounced (and so cannot be used for reading exception words aloud correctly, since such  words disobey the rules: if you apply the rules to read an exception word aloud, you will produce an incorrect pronunciation, such as reading pint as if it rhymed with mint ,hint ,lint , etc.).
If the reading system does include these two separate processing components, it might be possible that neurological damage could impair one component whilst leaving the other intact, to produce a specific pattern of acquired dyslexia.
Let us explore this possibility further.
Suppose the lexical procedure had been damaged whilst the non-lexical procedure remained intact.
What effect would this have on the reading system as a whole?
 Non-words and regular words could still be read aloud accurately (because the non-lexical procedure can handle both these types of stimuli correctly); but exception words would suffer.
If the patient cannot use the lexical procedure to read, say,pint , and so has to fall back upon the non-lexical procedure, a reading error will result:pint will be read with a short i (as in mint ).
Thus the pattern we would expect to see if the lexical procedure is damaged but the non-lexical procedure remains intact — if these procedures really are separate — is good reading of regular words and non-words with bad reading of exception words.
Furthermore, the wrong reading responses with exception words should demonstrate the application of rules to these words, as in the pint example.
Such errors are known as regularisation errors.
This pattern of acquired dyslexia is in fact seen.
It is known as surface dyslexia (Marshall and Newcombe, 1973; Coltheart, Masterson, Byng, Prior and Riddoch, 1983; Patterson, Marshall and Coltheart, 1985).
Thus surface dyslexia is one of a set of syndromes of acquired dyslexia, just as Broca's aphasia is one of a set of syndromes of acquired aphasia.
9.4.1 Surface dyslexia
The clearest case of surface dyslexia so far described is that of Bub, Cancelliere and Kertesz (1985).
This patient was actually normal at reading regular words and non-words (in terms of accuracy and reading time), but made many errors in reading exception words.
Almost all her errors here were regularisations: for example, she read have as if it rhymed with ‘cave ’,lose as if it rhymed with ‘hose ’,own as if it rhymed with ‘down ’, and steak as if it rhymed with ‘beak’.
FIGURE 16 An information processing model of language
Precisely where, within the lexical procedure for reading aloud, should we locate the defect that gives rise to surface dyslexia?
The model we set out earlier in Chapter 6 is reproduced here in Figure 16.
Whereabouts in the model would damage cause a patient to have to rely on the non-lexical procedure for reading?
There are at least two answers to this question.
The first is the visual word-recognition system: if a word cannot be processed by this system, it cannot be read aloud by lexical means.
The second is the spoken-word production system: even if a word is correctly identified by the visual word-recognition system, it cannot be read aloud lexically unless its pronunciation can be generated from the spoken-word production system.
There are other possibilities too (e.g. if both the semantic system and the direct link from visual word recognition to spoken word production are damaged reading via the lexical procedure will be impaired).
Which of these various possibilities provides the correct interpretation for surface dyslexia?
The question can be answered by using homophones : printed words with the same pronunciations but different spellings and different meanings, such as frays and phrase .
Suppose you present a surface dyslexic with the stimulus frays and ask, not for the word to be read aloud, but for the patient to say what the word means .
If the patient has a defective visual word-recognition system which fails to identify frays , then the word cannot get to the semantic system via the visual word-recognition system, so this, the usual route for reading comprehension, will fail.
However, the patient can still apply the non-lexical procedure to derive a pronunciation for frays , and then could use the auditory word recognition system to understand this spoken form.
But now a problem would arise: since frays and phrase have the same spoken form, how could the patient decide which of these two meanings is correct?
This reasoning leads to the prediction that, if the locus of impairment in surface dyslexia is within the visual word recognition system, the patient will sometimes confuse one homophone with another when asked to define single printed words — will say, for example, that the printed word frays means ‘a part of a sentence’.
By exactly the same reasoning, it would be predicted that a patient who reads pint with a short i (a regularisation error) would fail to understand this word, because the only route to understanding print is via pronunciation, and the patient's pronunciation represents a non-word (pint with a short i ).
On the other hand, if the deficit is located within the spoken word production system, the patient will be able to get from print to semantics correctly.
So frays will not be confused with phrase , and pint will be understood correctly as referring to beer or milk, even if it is read aloud incorrectly.
So which of these two patterns does one actually see in surface dyslexia?
The answer is both.
In some patients (e.g. Coltheart et al., 1983), only the first pattern is seen, so that the way a printed word is comprehended is via pronunciation; in others (e.g. Kay and Patterson, 1983) one does see examples where a word is read aloud wrongly but comprehended correctly.
This shows that even a single symptom (regularisation errors in reading aloud) can arise in different ways in different patients.
We have already argued that the concept of a syndrome is not of use as far as theoretical analysis of language disorders is concerned, because it is not the case that a small number of syndromes (defined as sets of symptoms which always co-occur) actually exist.
It is much more nearly true to say that no two patients ever show exactly the same constellation of symptoms.
We have now shown that, even for a single symptom shared by a set of patients the explanation of that symptom might be different for different patients.
So not only is it unwise to try to group patients, in terms of syndromes, it is not even possible to group them in terms of symptoms — that is, even a set of patients all showing the same symptom may not be homogeneous even with respect to that symptom.
How, then, can researchers generalise from one patient to others?
By referring the patients to a specific model of language processing.
In our surface dyslexia example, it does not matter that there are different kinds of surface dyslexics, so long as each of the patterns of surface dyslexia can be interpreted with reference to the same model of reading.
If this cannot be done, then that counts as evidence against whatever model one is using.
Although we have illustrated these points with reference to surface dyslexia, they are generally applicable: they apply to the investigation of any form of language disorder — expressive or receptive, spoken language or written language.
That is why we have digressed.
To return to the main point: we introduced surface dyslexia as the pattern of acquired dyslexia which would be expected if neurological damage had affected the lexical procedure  for reading aloud and spared the non-lexical procedure.
What of the opposite pattern?
If the lexical procedure were spared and the non-lexical procedure damaged, what symptoms should follow?
It should be obvious, from the discussion of dual-route reading models in Chapter 6, that the answer is that all words (exception or regular) would be read well, but non-words would not.
This pattern of acquired dyslexia is also seen: it is known as phonological dyslexia .
9.4.2 Phonological dyslexia
The clearest case of phonological dyslexia so far described is that of Funnell (1983a).
Her patient, WB, was almost normal at reading words, his accuracy being 85–90 per cent(even with complex words like satirical or preliminary ).
In contrast, he could not read non-words aloud at all, scoring 0 per cent correct even with simple non-words like cobe or nust .
The same pattern in less extreme form is shown by patients described by Beauvois and Derouesne (1979), Shallice and Warrington (1980) and Patterson (1982).
These two disorders, surface dyslexia and phonological dyslexia, have extremely straightforward interpretations in terms of the model of reading we set out in Chapter 6.
The two other forms of acquired dyslexia which have been clearly defined are, for different reasons, less straightforward to interpret, these disorders being letter-by-letter reading and deep dyslexia .
9.4.3 Letter-by-letter reading
This form of acquired dyslexia is sometimes called pure alexia or alexia without agraphia , the reason being that, as a rule, these patients have no impairment of writing or spelling, and so they contrast with patients having other kinds of acquired dyslexia, all of whom have had impaired writing and spelling as well as impaired reading.
The letter-by-letter reader will usually be able to write a passage well, and then be unable to read what he has written.
The reading difficulty is not due to some very peripheral visual defect, since the patient may be perfect at identifying letters (e.g. Warrington and Shallice, 1980).
The disorder gets its name from the way the patient behaves when trying to read a word: the letters of the word will usually be named aloud, from left to right, very slowly.
This process might take minutes if a word is long.
If the patient gets to the end of the word having named all the letters correctly, the word is likely then to be read aloud correctly.
An illustrative case of this form of acquired dyslexia is described by Patterson and Kay (1982).
How should this disorder be interpreted in relation to a model of normal reading?
Clearly, the problem is somewhere at a very early stage of the reading system.
The problem is that there is no stage in the model in Figure 16 where damage should cause the specific behaviour of identifying words by slow serial identification of their individual letters.
Perhaps the best one can do is to say that communication from the letter detector to the word detector level, which is normally both fast and parallel, has become slow and serial, but this is a rather ad hoc explanation, and certainly much less satisfying than the interpretations offered by the model for surface dyslexia and phonological dyslexia.
9.4.4 Deep dyslexia
In this kind of acquired dyslexia, described for example by Marshall and Newcombe (1966, 1973) and by Coltheart, Patterson and Marshall (1980), the most striking symptom is the semantic error in reading aloud.
The patient, asked to read aloud the single printed word artist , produces the response ‘picture’, or reads sepulchre as ‘tomb’.
Deep dyslexics exhibit several other reading symptoms too.
Non-words cannot be read aloud at all; abstract words like truth or equality are much less likely to be read aloud correctly than concrete words like blood or cathedral ; visual errors such as reading bush as ‘brush’ or forge as ‘ford’occur; function words, and the prefixes and suffixes on affixed words, cause particular difficulties.
Two approaches have been taken to the interpretation of deep dyslexia in relation to models of normal reading.
It is clear that the complex pattern of symptoms could not be a consequence of just one or two loci of damage within the normal reading system, and Morton and Patterson (1980) have offered an interpretation in which the disorder is explained as occurring when a number of  different loci in the normal reading system are all damaged.
In contrast, Coltheart (1980) and Saffran, Bogyo, Schwartz and Marin (1980) suggested that in deep dyslexia the normal reading system cannot operate at all, and that the reading that the patient can manage is mediated by an alternative processing system located in the right hemisphere, a system which might play no role at all in normal reading.
This issue is still unresolved: for further discussion, see Coltheart (1984), Patterson and Besner (1984a, b), Rabinowicz and Moscovitch (1984), and Zaidel and Schweiger (1984).
Having described the four best-documented forms that acquired dyslexia can take, we will conclude this chapter with a brief discussion of acquired dysgraphia.
The discussion will be brief because much less work has so far been done on acquired dysgraphia than on acquired dyslexia.
9.5 Acquired dysgraphia
The basic considerations which in Chapter 6 led to the view that the reading system involves two different processing procedures, one lexical and the other non-lexical, apply also to the spelling system.
We must be able to spell without relying on access to stored lexical information because we can spell non-words, and could not have learned their spellings beforehand.
But spelling cannot solely rely on this route: to be able to spell words like yacht or choir we must have access to specific lexical information about how each word is spelled.
So current models of spelling (e.g. Ellis, 1982) frequently include two procedures for going from speech to print, a lexical spelling procedure (retrieving the correct spelling from a lexicon of spellings) and a non-lexical spelling procedure (applying a system of rules specifying correspondences between sounds and letters).
It is not surprising, then, that very recently varieties of dysgraphia have been recognised in which there are various patterns of impairment to one or other of these two routes.
In phonological dysgraphia , the spelling of real words is preserved whilst the spelling of non-words is impaired.
For example, the patient described by Shallice (1981 b) could spell more than 90 per cent of the words dictated to him, but could produce plausible spellings for almost no non-words.
In the converse  disorder, which unfortunately has been given several different names (surface dysgraphia ,phonological spelling , and lexical agraphia ), the spelling to dictation of non-words is good, but words are commonly misspelled, and the misspellings are usually phonologically correct — e.g. ‘search’—surch , ‘spade’—spaid and ‘come’—cumme .
Hatfield and Patterson (1983) describe a representative case of this kind of acquired dysgraphia.
It is clear that, within the framework of a dual-route model of spelling, phonological dysgraphia is the pattern you would expect if the non-lexical procedure for spelling is selectively damaged, and surface dysgraphia is the pattern expected if it is the lexical procedure for spelling that is the damaged one.
Other patterns of acquired dysgraphia are beginning to be identified too.
We will mention one, called deep dysgraphia because of its analogy to deep dyslexia.
A patient described by Bub and Kertesz (1982) made semantic errors in writing to dictation; for example‘desk' —chair , ‘give’—take and ‘yacht’—boat .
The patient could not write non-words to dictation, spelled concrete nouns better than abstract nouns, and was poor at spelling function words.
None of these impairments appeared when the patient was asked to read words and non-words rather than to spell them.
9.6 Overview
According to the ideas outlined in Chapter 6, the language processing system is made up of a number of information-processing modules , each responsible for a specific information-processing task (such as recognising letters, for example, or producing spoken words).
In Chapter 6, we used this approach to the understanding of language processing to interpret the results of various different sorts of experiments involving the comprehension and production of single words in skilled users of language.
This general approach can also be used to interpret the patterns of impairment to language processing seen in patients with acquired disorders of language produced by neurological damage.
If the sentence production system includes one component whose task is to retrieve specific content words from the mental lexicon, and another component whose task is to create a linguistic  structure into which these content words are to be inserted, then one would not be surprised to find some patients whose speech consists of unstructured sequences of specific content words, and others whose speech has correct linguistic structure but lacks specific content words.
We saw in Chapter 8 that Garrett's model of sentence production does contain these two components; and the examples of spontaneous speech by aphasic patients we gave in sections 9.2.1 and 9.2.2 illustrate the two patterns which occur if one of the components is damaged and the other spared.
Similarly, models of normal reading aloud which distinguish a lexical from a non-lexical procedure for reading aloud provide straightforward interpretations of two contrasting patterns of acquired dyslexia (surface dyslexia and phonological dyslexia), and models of normal spelling which distinguish lexical from non-lexical procedures for spelling allow us to interpret two contrasting patterns of acquired dysgraphia (surface dysgraphia and phonological dysgraphia).
If you think back to Chapter 5 where we discussed developmental disorders of language, you will recall that the situation there was rather similar.
It was possible to establish plausible relationships between what is known about normal language acquisition, especially learning to read, and what is known about developmental disorders of language, especially developmental dyslexia.
Thinking about language acquisition and skilled language use in terms of the particular forms of information-processing that go on when any linguistic task is being performed, and thinking about language disorders explicitly as patterns of impairments and preservations of different forms of information-processing, thus provides a particularly powerful way of illuminating both normal and disordered linguistic capacities.
It is to be hoped that future research into the nature of language processing will continue to relate findings about the performance of both skilled and impaired language users on the wide range of tasks which go together to make up out ability to acquire and use language — for it is only by attempting to do so that it will ultimately be possible for psychologists to develop a comprehensive model of language processing.